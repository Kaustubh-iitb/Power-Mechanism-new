{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = \"Data/Twitter_data/twitter_training.csv\"\n",
    "# FILEPATH: /c:/Users/Kaustubh/Downloads/Data_preproc_twitter.ipynb\n",
    "\n",
    "\n",
    "# Read the CSV data\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Create X and Y as 'Review' and 'Sentiment' respectively\n",
    "X_text = data['Review'].values\n",
    "Y_sent = data['Sentiment'].values\n",
    "Y_label = []\n",
    "# Initialize the LabelEncoder\n",
    "Y_dict = {}\n",
    "for y in Y_sent:\n",
    "    if y not in Y_dict:\n",
    "        Y_dict[y] = len(Y_dict)\n",
    "    Y_label.append(Y_dict[y])\n",
    "Y_tensor = torch.tensor(Y_label)\n",
    "X_list = []\n",
    "for x in X_text:\n",
    "    X_list.append(str(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surajracha/miniconda3/envs/chatllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "encoder_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X = encoder_model.encode(X_list, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: torch.Size([59745, 384]) torch.Size([59745])\n",
      "Test set shape: torch.Size([14937, 384]) torch.Size([14937])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test sets\n",
    "print(\"Train set shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: torch.Size([59745, 384]) torch.Size([59745])\n",
      "Test set shape: torch.Size([14937, 384]) torch.Size([14937])\n",
      "Epoch [1/1000], Loss: 1.3869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/1000], Loss: 0.9766\n",
      "Epoch [201/1000], Loss: 0.8432\n",
      "Epoch [301/1000], Loss: 0.6415\n",
      "Epoch [401/1000], Loss: 0.4331\n",
      "Epoch [501/1000], Loss: 0.2857\n",
      "Epoch [601/1000], Loss: 0.1981\n",
      "Epoch [701/1000], Loss: 0.1481\n",
      "Epoch [801/1000], Loss: 0.1193\n",
      "Epoch [901/1000], Loss: 0.1025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the train and test sets\n",
    "print(\"Train set shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, Y_test.shape)\n",
    "\n",
    "# Check if CUDA is available\n",
    "\n",
    "\n",
    "# Move the tensors to the GPU\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the network architecture\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X.shape[1]  # Number of input features\n",
    "hidden_size = 500  # Number of neurons in the hidden layer\n",
    "num_classes = len(Y_dict)  # Number of output classes\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the network\n",
    "model = SimpleNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Convert Y_tensor to long ty\n",
    "# Update the model and criterion to use GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train).to(device)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(epoch%100==0):\n",
    "        \n",
    "\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test set: 87.32007765950326%\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels\n",
    "# Predict the labels for the test set\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test.to(device))\n",
    "    _, test_predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "# Calculate the accuracy on the test set\n",
    "test_correct = (test_predicted == Y_test.to(device)).sum().item()\n",
    "test_total = Y_test.size(0)\n",
    "test_accuracy = test_correct / test_total\n",
    "\n",
    "print('Accuracy of the model on the test set: {}%'.format(test_accuracy * 100))\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super(Net, self ).__init__()\n",
    "        \n",
    "        self.loss_reg = 0\n",
    "        self.p =p \n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        self.H_net1 = nn.Sequential(\n",
    "            nn.Linear(384, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(256, 384*384).to(device)\n",
    "        )\n",
    "        self.X_net = nn.Sequential(\n",
    "            nn.Linear(384, 512),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(64, 128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(512, 4),\n",
    "            nn.Softmax(dim=2)\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        def H_mul(z):\n",
    "            H12 = self.H_net1(z)\n",
    "            H12= H12.reshape(z.shape[0],d,d)\n",
    "            x12 = torch.matmul(z,H12)\n",
    "            return(x12)\n",
    "    \n",
    "        \n",
    "        def batch_jacobian(func, z, create_graph=False):\n",
    "            # x in shape (Batch, Length)\n",
    "            def _func_sum(z):\n",
    "                return func(z).sum(dim=0)\n",
    "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
    "        \n",
    "        x.requires_grad =True\n",
    "        p = self.p\n",
    "        self.x = x\n",
    "        d = x.shape[1]\n",
    "        bs = x.shape[0]\n",
    "        x= torch.unsqueeze(x,1)\n",
    "        z = x.to(device)\n",
    "        loss_reg = torch.zeros(bs,d).to(device)\n",
    "        for i in range(p):\n",
    "            H = self.H_net1(z).to(device)\n",
    "            H = H.reshape(bs,d,d)\n",
    "            z = torch.matmul(z,H).to(device)\n",
    "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
    "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
    "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).to(device)\n",
    "        self.loss_reg = loss_reg\n",
    "        self.y = z\n",
    "        y = self.X_net(z)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_priv(net,trainloader,optimizer,epochs,h,rate=10,device= torch.device('cuda'),print_cond = True,only_reg_flag=0,lr_schedular =None,lambda_loss=1):\n",
    "    # scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    print(\"Training\")\n",
    "    lr = lr_schedular\n",
    "    # net = net.to(device)\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        \n",
    "        # scheduler.step()\n",
    "        running_loss = 0.0\n",
    "        running_loss_reg = 0.0\n",
    "        if(lr):\n",
    "       \n",
    "            for groups in optimizer.param_groups:\n",
    "                groups['lr'] = lr(epoch)\n",
    "        # optimizer.param_groups[0]['lr'] = lr(epoch)\n",
    "        \n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            print(\"Starter Loop: \",i)\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            bs = len(data[0])\n",
    "            \n",
    "            inputs = data[0].to(device)\n",
    "            inputs.requires_grad = True\n",
    "            labels = data[1].to(device)\n",
    "            print(inputs.shape,labels.shape)\n",
    "            f = py_kde(inputs,inputs,h)\n",
    "            f_der = py_kde_der(f,inputs)\n",
    "            print(f.shape,f_der.shape)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            print(outputs.shape)\n",
    "            if(only_reg_flag==1):\n",
    "                loss = torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum()\n",
    "            elif(only_reg_flag==2):\n",
    "                loss = criterion(torch.squeeze(outputs),torch.squeeze(labels))\n",
    "                \n",
    "            else:\n",
    "                loss = lambda_loss*bs*criterion(torch.squeeze(outputs),torch.squeeze(labels)) + torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum()\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            optimizer.step()\n",
    "            loss = loss.detach().cpu()/len(inputs)\n",
    "\n",
    "          \n",
    "            if(epoch ==0 and i==0):\n",
    "                continue\n",
    "      \n",
    "            # loss_reg = torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum().detach().cpu()/len(inputs)\n",
    "            # wandb.log({\"loss\": loss.item(),\"loss_reg\":loss_reg.item()})\n",
    "\n",
    "            # print statistics\n",
    "            # print(loss.sum().shape)\n",
    "            running_loss += loss.item()\n",
    "            running_loss_reg += torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum().item()\n",
    "           # print every 2000 mini-batches\n",
    "            if((i+1)%rate==0):\n",
    "\n",
    "                if(print_cond):\n",
    "                    \n",
    "                    print(\"Epoch: \",epoch + 1,\"Loss: \" ,running_loss /(rate*trainloader.batch_size),\"Reg Loss: \",running_loss_reg /(rate*trainloader.batch_size))\n",
    "                    running_loss = 0.0\n",
    "                    running_loss_reg = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gau_ker(u):\n",
    "    return torch.pow(2*torch.tensor(torch.pi),u.shape[1]/(-2))*torch.exp(torch.bmm(u.view(u.shape[0], 1, u.shape[1]), u.view(u.shape[0],  u.shape[1],1))/(-2)).to(device)\n",
    "\n",
    "def py_kde(x,X_t,h):\n",
    "    norm = X_t.shape[0]*(h**x.shape[1])\n",
    "    prob = torch.zeros(x.shape[0]).to(device)\n",
    "    for i in range(len(X_t)):\n",
    "        prob+= (torch.squeeze(gau_ker((x - X_t[i])/h))/norm).to(device)\n",
    "    return(prob)\n",
    "\n",
    "\n",
    "def py_kde_der(p_x,x):\n",
    "    # x.requires_grad = True\n",
    "    # p_x = py_kde(x,X_t,h)\n",
    "    return (torch.autograd.grad(p_x,x,torch.ones_like(p_x),allow_unused=True,create_graph=True)[0]).to(device)\n",
    "\n",
    "\n",
    "def gau_ker_der(X,h):\n",
    "    N= X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    grad = torch.zeros(X.shape)\n",
    "    for n in range(N):\n",
    "        for i in range(d):\n",
    "            for j in range(N):\n",
    "                grad[n][i]+= torch.exp(-1*torch.dot((X[n]-X[j]),(X[n]-X[j]))/(2*h*h))*(X[n][i] -X[j][i]) /(N*(h**(d+2))*((2*math.pi)**(d/2)))\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 384]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "batch_size_priv = 64\n",
    "net_depth = 1\n",
    "\n",
    "# Move the tensors to the GPU\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "Y_test = Y_test.to(device)\n",
    "train_priv = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "test_priv = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "\n",
    "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=batch_size_priv,\n",
    "                                        shuffle=True, num_workers=2, drop_last=True)\n",
    "testloader_priv = torch.utils.data.DataLoader(test_priv, batch_size=batch_size_priv,\n",
    "                                        shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "\n",
    "net = Net(net_depth)\n",
    "print(sum(p.numel() for p in net.X_net.parameters() if p.requires_grad))\n",
    "\n",
    "optim = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# wandb.watch(net)\n",
    "train_flag = 0\n",
    "lr_schedule = None\n",
    "\n",
    "\n",
    "train_model_priv(net, trainloader_priv, optim, num_epochs, h=0.82, rate=1, device=device, only_reg_flag=train_flag, lr_schedular=lr_schedule,lambda_loss=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
