{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import dcMinMaxFunctions as dc\n",
    "# import dcor\n",
    "from scipy.misc import derivative\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import stats\n",
    "import wandb\n",
    "from opacus import PrivacyEngine\n",
    "from cov_help import *\n",
    "import time\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = cov_data_loader(\"data/covtype.csv\",norm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "batch_size = 4096\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train,Y_train), batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=2,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test,Y_test), batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "#write code to append Xemb and Xemb_rest\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = nn.Sequential(\n",
    "        nn.Linear(54, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 7),\n",
    "        nn.Softmax(dim=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# do\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# from pvc import Dataset\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from stl.lightning.loggers import ManifoldTensorBoardLogger\n",
    "# from stl.lightning.callbacks.all import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# do\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# from opacus import PrivacyEngine\n",
    "# from opacus.data_loader import DPDataLoader\n",
    "# from opacus.validators.module_validator import ModuleValidator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "-3.0\n",
      "[torch.Size([784, 10])]\n",
      "tensor([-0.2900, -0.2900, -0.2900,  ..., -0.2900, -0.2900, -0.2900],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[ 1.2115, -0.7429, -0.8727,  ..., -0.3220,  0.9429,  0.7288],\n",
      "        [ 0.3995,  0.3587, -0.5243,  ...,  0.4212, -1.0353, -0.8312],\n",
      "        [ 0.3912,  0.4951,  0.6812,  ...,  1.4518,  0.2833,  1.4999],\n",
      "        ...,\n",
      "        [ 1.2337, -0.5575,  1.5038,  ...,  0.2897, -0.6609, -0.8485],\n",
      "        [-0.9495,  1.0158, -0.0466,  ..., -0.4603, -0.5817,  1.8481],\n",
      "        [ 0.4128, -1.4541, -1.0309,  ..., -0.4845,  0.9611, -0.5176]],\n",
      "       requires_grad=True) tensor([[ 1.2115, -0.7429, -0.8727,  ..., -0.3220,  0.9429,  0.7288],\n",
      "        [ 0.3995,  0.3587, -0.5243,  ...,  0.4212, -1.0353, -0.8312],\n",
      "        [ 0.3912,  0.4951,  0.6812,  ...,  1.4518,  0.2833,  1.4999],\n",
      "        ...,\n",
      "        [ 1.2337, -0.5575,  1.5038,  ...,  0.2897, -0.6609, -0.8485],\n",
      "        [-0.9495,  1.0158, -0.0466,  ..., -0.4603, -0.5817,  1.8481],\n",
      "        [ 0.4128, -1.4541, -1.0309,  ..., -0.4845,  0.9611, -0.5176]],\n",
      "       requires_grad=True)\n",
      "tensor([[ 1.3540, -0.3793, -1.3447,  ..., -0.1595, -2.4384, -0.1393],\n",
      "        [ 0.4551, -0.9861,  1.9324,  ..., -0.7311, -1.1460, -1.1899],\n",
      "        [-1.3280, -1.3214, -0.1361,  ..., -1.0172, -1.4923, -0.3444],\n",
      "        ...,\n",
      "        [ 0.4376, -2.1039,  0.2817,  ...,  0.8467,  0.9692,  1.1916],\n",
      "        [-1.2303,  0.1924, -0.7091,  ..., -0.9724, -0.5448,  0.2978],\n",
      "        [ 1.5700, -1.4436, -0.4908,  ...,  0.3098, -1.4827, -2.2172]],\n",
      "       requires_grad=True) tensor([[ 1.3540, -0.3793, -1.3447,  ..., -0.1595, -2.4384, -0.1393],\n",
      "        [ 0.4551, -0.9861,  1.9324,  ..., -0.7311, -1.1460, -1.1899],\n",
      "        [-1.3280, -1.3214, -0.1361,  ..., -1.0172, -1.4923, -0.3444],\n",
      "        ...,\n",
      "        [ 0.4376, -2.1039,  0.2817,  ...,  0.8467,  0.9692,  1.1916],\n",
      "        [-1.2303,  0.1924, -0.7091,  ..., -0.9724, -0.5448,  0.2978],\n",
      "        [ 1.5700, -1.4436, -0.4908,  ...,  0.3098, -1.4827, -2.2172]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: clip norm\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, depth: int, value: float, efficient: bool):\n",
    "        \"\"\"\n",
    "        implements the nodes of the tree. if the tree is efficient we will weigh\n",
    "        the nodes by the weight method. adapted from opacus privacy engine.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.value = value\n",
    "        self.efficient = efficient\n",
    "\n",
    "    def update_value(self, indx, new_value):\n",
    "        self.value[indx] += new_value\n",
    "\n",
    "    def get_value(self, indx):\n",
    "        \"\"\"as indicated by the efficient implementation of tree aggregation\n",
    "        we reweight it by 1 / (2 - 2^(-depth)) where depth starts from the root.\n",
    "        ex. depth(root) = 0\n",
    "        \"\"\"\n",
    "        if self.efficient:\n",
    "            # print('dd', self.depth)\n",
    "            # print('efficient test', float((1.0 / (2 - math.pow(2, -self.depth))) ** 0.5) * self.value)\n",
    "            return ((1.0 / (2 - math.pow(2, -self.depth))) ** 0.5) * self.value[indx]\n",
    "        else:\n",
    "            return 1.0 * self.value[indx]\n",
    "\n",
    "\n",
    "class TreeAggregation:\n",
    "    \"\"\"\n",
    "    Correlated noise added to gradient sums from a private binary tree\n",
    "    Efficient Implementation Paper (reduces variance in sum of gaussian noise values):\n",
    "        Efficient Use of Differentially Private Binary Trees\n",
    "        https://privacytools.seas.harvard.edu/files/privacytools/files/honaker.pdf\n",
    "    Vanilla Implementation:\n",
    "        Differential Privacy Under Continual Observation\n",
    "        http://www.wisdom.weizmann.ac.il/~naor/PAPERS/continual_observation.pdf\n",
    "\n",
    "    Args:\n",
    "        std = standard deviation of gaussian noise\n",
    "        efficient = decides which kind of tree aggregation to be used\n",
    "        seed = added for reproducibility\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grad_sizes,\n",
    "        std: float = 1.0,\n",
    "        efficient: bool = False,\n",
    "        seed: int = 0,\n",
    "        mode: str = \"prod\",  # alternatives are prod/test\n",
    "    ):\n",
    "        self.efficient = efficient\n",
    "        self.std = std\n",
    "        self.seed = seed\n",
    "        self.mode = mode\n",
    "        self.grad_sizes = grad_sizes\n",
    "\n",
    "        self.step = 1\n",
    "        self.depth = 0  # TODO: increase depth appropriately\n",
    "        self.max_nodes = 2 ** (self.depth)\n",
    "        self.tree = []\n",
    "\n",
    "        # reproducible experiments\n",
    "        self.generator = torch.Generator()\n",
    "        self.generator.manual_seed(self.seed)\n",
    "\n",
    "    def add_to_tree_helper(\n",
    "        self,\n",
    "        gradient,\n",
    "        noise_to_add,\n",
    "        binary_repr_step,\n",
    "        step,\n",
    "        max_nodes,\n",
    "        add_gradient,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        adds noise to each node on the path from the root node to the leaf node.\n",
    "        Also, returns correlated noise for this specific path to be added to the FTRL optimizer.\n",
    "\n",
    "        Args:\n",
    "            noise_to_add = torch Tensor (gaussian noise) to be added\n",
    "            binary_repr_step = convert current step value to binary representation\n",
    "            step = number of times this function has been called (correlated with maximum nodes in the system)\n",
    "            max_nodes = maximum possible nodes in the tree (i.e. counted after assuming a complete binary tree,\n",
    "                where each node except the leaf node have exactly two childen)\n",
    "\n",
    "        Returns:\n",
    "            correlated noise determined from the current tree structure & step number\n",
    "        \"\"\"\n",
    "        if len(self.tree) == 0:\n",
    "            # print('Root Added')\n",
    "            noise_sum = [torch.zeros(grad_size) for grad_size in self.grad_sizes]\n",
    "            self.tree.append(\n",
    "                TreeNode(depth=self.depth, value=noise_to_add, efficient=self.efficient)\n",
    "            )\n",
    "            # print(len(self.tree), gradient, len(gradient))\n",
    "            if add_gradient:\n",
    "                for indx in range(len(self.grad_sizes)):\n",
    "                    self.tree[0].update_value(indx, gradient[indx])\n",
    "\n",
    "            for indx in range(len(self.grad_sizes)):\n",
    "                noise_sum[indx] += self.tree[0].get_value(indx)\n",
    "            return noise_sum\n",
    "\n",
    "        # node_indx = 0\n",
    "        binary_step_indx = 1\n",
    "\n",
    "        # append noise & init noise_sum\n",
    "        self.tree.append(\n",
    "            TreeNode(depth=self.depth, value=noise_to_add, efficient=self.efficient)\n",
    "        )\n",
    "        noise_sum = [torch.zeros(grad_size) for grad_size in self.grad_sizes]\n",
    "\n",
    "        # for indx in range(len(self.grad_sizes)):\n",
    "        #    self.tree[step].update_value(indx, gradient[indx])\n",
    "\n",
    "        # get back value\n",
    "        if binary_repr_step[0] == \"1\":  # test if left node\n",
    "            for indx in range(len(self.grad_sizes)):\n",
    "                noise_sum[indx] += self.tree[step].get_value(indx)\n",
    "\n",
    "        steps = [step]\n",
    "        while step >= 0 and binary_step_indx < len(binary_repr_step):\n",
    "            left_indx = int((step - 1) / 2)\n",
    "            if step % 2 == 1:  # is left child\n",
    "                step = int((step - 1) / 2)\n",
    "            else:  # is right node\n",
    "                step = int((step - 2) / 2)\n",
    "            steps.append(step)\n",
    "            if add_gradient:\n",
    "                for indx in range(len(self.grad_sizes)):\n",
    "                    self.tree[step].update_value(indx, gradient[indx])\n",
    "\n",
    "            # get value\n",
    "            if binary_repr_step[binary_step_indx] == \"1\" and left_indx < len(self.tree):\n",
    "                for indx in range(len(self.grad_sizes)):\n",
    "                    noise_sum[indx] += self.tree[left_indx].get_value(indx)\n",
    "\n",
    "            binary_step_indx += 1\n",
    "\n",
    "        # print(steps)\n",
    "        return noise_sum\n",
    "\n",
    "    def print_tree(self):\n",
    "        print(\"------------------------Tree Starts------------------------\")\n",
    "        for indx in range(len(self.tree)):\n",
    "            print(\n",
    "                \"(\" + str(indx + 1) + \",\" + str(self.tree[indx].value[0].item()) + \")\",\n",
    "                end=\" \",\n",
    "            )\n",
    "        print(\"\\n------------------------Tree Ends------------------------\")\n",
    "\n",
    "    def add_to_tree_and_get_sum(\n",
    "        self,\n",
    "        gradient: torch.Tensor,\n",
    "        test_noise: Optional[torch.Tensor] = None,\n",
    "        add_gradient: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        add the new value to each node along the path\n",
    "\n",
    "        Args:\n",
    "            test_noise = torch.normal(mean=0.0, std=self.std, generator=torch.Generator())\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if self.mode == \"prod\":\n",
    "            noise_to_add = []\n",
    "            for indx in range(len(self.grad_sizes)):\n",
    "                # print('size', self.grad_sizes[indx])\n",
    "                noise_to_add.append(\n",
    "                    torch.normal(\n",
    "                        mean=0.0,\n",
    "                        std=self.std,\n",
    "                        size=self.grad_sizes[indx],\n",
    "                        # generator=self.generator\n",
    "                    )\n",
    "                )\n",
    "        elif self.mode == \"test\":\n",
    "            noise_to_add = test_noise\n",
    "\n",
    "        noise_sum = self.add_to_tree_helper(\n",
    "            gradient,\n",
    "            noise_to_add,\n",
    "            np.binary_repr(self.step)[::-1],\n",
    "            self.step - 1,\n",
    "            self.max_nodes + 1,\n",
    "            add_gradient,\n",
    "        )\n",
    "\n",
    "        self.step += 1\n",
    "        # print('metric', self.step, self.max_nodes, self.depth)\n",
    "        if self.step > self.max_nodes:\n",
    "            self.depth += 1\n",
    "            self.max_nodes += 2 ** (self.depth)\n",
    "\n",
    "        return noise_sum\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# do\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "\n",
    "class FTRLM(Optimizer):\n",
    "    \"\"\"\n",
    "    implements FTRL optimizer to be used with Opacus or alternative noise generation\n",
    "    mechanism with (optional) momentum\n",
    "\n",
    "    DP-FTRL (private) paper: https://arxiv.org/abs/2103.00039\n",
    "    FTRL (non-private) paper: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,  # TODO: add tree aggregator\n",
    "        model_param_sizes: List[torch.Size] = [(1,)],\n",
    "        lr: float = 0.0,\n",
    "        momentum: float = 0.0,\n",
    "        dampening: float = 0.0,\n",
    "        nesterov: bool = False,\n",
    "        noise_std: float = 0.0,\n",
    "        max_grad_norm: float = 100.0,\n",
    "        seed: int = 0,\n",
    "        efficient: bool = False,\n",
    "        is_PCA_enabled: bool = False,\n",
    "        PCA_variance: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params = parameters for torch optimizer\n",
    "            lr = learning rate\n",
    "            momentum = # TODO: add implementation\n",
    "            nesterov = # TODO: add implementation\n",
    "            dampening = # # TODO: add implementation\n",
    "            noise_std = amount of noise to be added to teh\n",
    "        \"\"\"\n",
    "\n",
    "        # sanity checks for parameters\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(\"Invalid Learning Rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid Momentum Parameter: {}\".format(momentum))\n",
    "        if isinstance(max_grad_norm, float) and max_grad_norm <= 0:\n",
    "            raise ValueError(\n",
    "                \"max_grad_norm = {} is not a valid value. Please provide a float > 0.\".format(\n",
    "                    max_grad_norm\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.seed = seed\n",
    "        self.noise_std = noise_std\n",
    "        self.efficient = efficient\n",
    "        self.is_PCA_enabled = is_PCA_enabled\n",
    "        self.PCA_variance = PCA_variance\n",
    "\n",
    "        self.tree_aggregator = TreeAggregation(\n",
    "            grad_sizes=model_param_sizes,\n",
    "            std=self.noise_std,\n",
    "            efficient=self.efficient,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "\n",
    "        # TODO: add weight decay\n",
    "        # if not 0.0 < weight_decay:\n",
    "        #    raise ValueError(\"Invalid Weight Decay Parameter: {}\".format(weight_decay))\n",
    "\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "\n",
    "        defaults = dict(\n",
    "            lr=lr, dampening=dampening, momentum=momentum, nesterov=nesterov\n",
    "        )\n",
    "\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def clip_gradient(self, grad):  # TODO: add per-example clipping\n",
    "        # print('Sanity Check', grad.shape)\n",
    "        if torch.norm(grad) > self.max_grad_norm:\n",
    "            return (grad * self.max_grad_norm) / float(torch.norm(grad))\n",
    "        return grad\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"performs one step of the DP-FTRL algorithm.\n",
    "\n",
    "        Args:\n",
    "            closure (callable, optional): a closure that reevaluates\n",
    "            the model and returns the loss\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        gradients = []\n",
    "        noisy_gradients = self.tree_aggregator.add_to_tree_and_get_sum(\n",
    "            gradients, add_gradient=False\n",
    "        )\n",
    "        for group in self.param_groups:\n",
    "            momentum = group[\"momentum\"]\n",
    "            dampening = group[\"dampening\"]\n",
    "            nesterov = group[\"nesterov\"]\n",
    "            lr = group[\"lr\"]\n",
    "            for p, noise in zip(group[\"params\"], noisy_gradients):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                d_p = p.grad\n",
    "                d_p = self.clip_gradient(d_p)\n",
    "                param_state = self.state[p]\n",
    "                if len(param_state) == 0:\n",
    "                    param_state[\"grad_sum\"] = torch.zeros_like(d_p)\n",
    "                    param_state[\"momentum\"] = torch.zeros_like(p)\n",
    "                    param_state[\"initial_model\"] = torch.zeros_like(p)\n",
    "                    param_state[\"initial_model\"].add_(p)  # only add initial model\n",
    "\n",
    "                initial_model, grad_sum = (\n",
    "                    param_state[\"initial_model\"],\n",
    "                    param_state[\"grad_sum\"],\n",
    "                )\n",
    "\n",
    "                grad_sum.add_(d_p)\n",
    "                # PCA\n",
    "                # noisy_gradient = noisy_gradients[indx]\n",
    "                \"\"\"if self.is_PCA_enabled:\n",
    "                    pca = PCA(self.PCA_variance)\n",
    "                    dimension2 = 1\n",
    "                    gradient_shape = noisy_gradient.shape\n",
    "                    for i in range(len(noisy_gradient.shape)):\n",
    "                        if i != 0:\n",
    "                            dimension2 *= gradient_shape[i]\n",
    "                    noisy_gradient_2D = noisy_gradient.reshape((gradient_shape[0], dimension2))\n",
    "                    noisy_transformed_gradient = pca.fit_transform(noisy_gradient_2D)\"\"\"\n",
    "\n",
    "                param_state[\"momentum\"].mul_(momentum).add_(\n",
    "                    grad_sum + noise, alpha=(1 - dampening)\n",
    "                )\n",
    "\n",
    "                # Nesterov\n",
    "                if nesterov:\n",
    "                    delta_w = (grad_sum + noise).add(\n",
    "                        param_state[\"momentum\"], alpha=momentum\n",
    "                    )\n",
    "                else:\n",
    "                    delta_w = param_state[\"momentum\"]\n",
    "\n",
    "                # p = initial_model\n",
    "                with torch.no_grad():\n",
    "                    p.copy_(initial_model - delta_w * lr)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# do\n",
    "import unittest\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class FTRLMTest(unittest.TestCase):\n",
    "    # init\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        # add multiple tests\n",
    "\n",
    "    # utils\n",
    "    def change_lr(self, opt, new_lr):\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = new_lr\n",
    "\n",
    "    def get_lr(self, opt):\n",
    "        for group in opt.param_groups:\n",
    "            return group[\"lr\"]\n",
    "\n",
    "    def get_momentum(self, opt):\n",
    "        for group in opt.param_groups:\n",
    "            return group[\"momentum\"]\n",
    "\n",
    "    # tests\n",
    "    def test_lr_deterministic(self):\n",
    "        param = torch.tensor([0.0], requires_grad=True)\n",
    "        param.grad = torch.tensor([1.0])\n",
    "        self.optimizer = FTRLM([param], lr=1, noise_std=0.0, max_grad_norm=10.0)\n",
    "        # self.optimizer.set_param_groups([param], lr=1)\n",
    "\n",
    "        # output = 0 - 1.0 * 1.0 = -1.0\n",
    "        self.assertAlmostEqual(self.get_lr(self.optimizer), 1.0)\n",
    "        self.optimizer.step()  # 1st step\n",
    "        print(param.item())\n",
    "        self.assertAlmostEqual(param.item(), -1.0, delta=1e-5)\n",
    "\n",
    "        # output = 0 - 1.5 * (1.0 + 1.0) = -3.0\n",
    "        self.change_lr(self.optimizer, 1.5)  #\n",
    "        self.assertAlmostEqual(self.get_lr(self.optimizer), 1.5)\n",
    "        self.optimizer.step()\n",
    "        print(param.item())\n",
    "        self.assertAlmostEqual(param.item(), -3.0, delta=1e-5)\n",
    "\n",
    "    def test_momentum(self):\n",
    "        param = torch.zeros((784, 10), requires_grad=True)\n",
    "        shapes = [param.shape]\n",
    "        # shapes = [p.shape for p in param]\n",
    "        print(shapes)\n",
    "        param.grad = torch.ones_like(param)\n",
    "        self.optimizer = FTRLM(\n",
    "            [param],\n",
    "            lr=0.1,\n",
    "            model_param_sizes=shapes,\n",
    "            noise_std=0.0,\n",
    "            momentum=0.9,\n",
    "            max_grad_norm=90.0,\n",
    "        )\n",
    "\n",
    "        self.assertAlmostEqual(self.get_lr(self.optimizer), 0.1)\n",
    "        self.assertAlmostEqual(self.get_momentum(self.optimizer), 0.9)\n",
    "        for epoch in range(2):\n",
    "            self.optimizer.step()\n",
    "        output = torch.flatten(param)\n",
    "        print(output)\n",
    "        self.assertTrue(\n",
    "            torch.allclose(\n",
    "                output,\n",
    "                torch.Tensor(\n",
    "                    [-0.29 * np.ones_like(p.detach().numpy()) for p in output]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def test_ftrl_similar_sgd(self):  # without nosie should perform simlarly to SGD\n",
    "        param_optimizer = torch.normal(\n",
    "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
    "        )\n",
    "        # param_optimizer = torch.zeros((784, 10))\n",
    "        param_optimizer.grad = torch.normal(\n",
    "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
    "        )\n",
    "        # print('init grad', param_optimizer.grad)\n",
    "        shapes = [param_optimizer.shape]\n",
    "        param_sgd = deepcopy(param_optimizer)\n",
    "        param_sgd.grad = deepcopy(param_optimizer.grad)\n",
    "        self.optimizer = FTRLM(\n",
    "            [param_optimizer],\n",
    "            model_param_sizes=shapes,\n",
    "            lr=0.01,\n",
    "            noise_std=0.0,\n",
    "            max_grad_norm=500.0,\n",
    "        )\n",
    "        self.sgd = torch.optim.SGD([param_sgd], lr=0.01)\n",
    "\n",
    "        for epoch in range(3):\n",
    "            self.optimizer.step()\n",
    "            self.sgd.step()\n",
    "        print((param_optimizer), (param_sgd))\n",
    "        self.assertTrue(\n",
    "            torch.allclose(torch.flatten(param_optimizer), torch.flatten(param_sgd))\n",
    "        )\n",
    "\n",
    "    def test_ftrl_similar_sgd_momentum(\n",
    "        self,\n",
    "    ):  # without nosie should perform simlarly to SGD\n",
    "        param_optimizer = torch.normal(\n",
    "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
    "        )\n",
    "        # param_optimizer = torch.zeros((784, 10))\n",
    "        param_optimizer.grad = torch.normal(\n",
    "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
    "        )\n",
    "        # print('init grad', param_optimizer.grad)\n",
    "        shapes = [param_optimizer.shape]\n",
    "        param_sgd = deepcopy(param_optimizer)\n",
    "        param_sgd.grad = deepcopy(param_optimizer.grad)\n",
    "        self.optimizer = FTRLM(\n",
    "            [param_optimizer],\n",
    "            model_param_sizes=shapes,\n",
    "            lr=0.01,\n",
    "            noise_std=0.0,\n",
    "            max_grad_norm=500.0,\n",
    "            momentum=0.9,\n",
    "        )\n",
    "        self.sgd = torch.optim.SGD([param_sgd], lr=0.01, momentum=0.9)\n",
    "\n",
    "        for epoch in range(3):\n",
    "            self.optimizer.step()\n",
    "            self.sgd.step()\n",
    "        print((param_optimizer), (param_sgd))\n",
    "        self.assertTrue(\n",
    "            torch.allclose(torch.flatten(param_optimizer), torch.flatten(param_sgd))\n",
    "        )\n",
    "\n",
    "\n",
    "test = FTRLMTest()\n",
    "test.test_lr_deterministic()  # DONE\n",
    "test.test_momentum()\n",
    "test.test_ftrl_similar_sgd()\n",
    "test.test_ftrl_similar_sgd_momentum()\n",
    "\n",
    "####DATALOADERS####\n",
    "\n",
    "####NET####\n",
    "\n",
    "\n",
    "# do\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=54, out_features=64, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (9): ReLU()\n",
       "  (10): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (11): ReLU()\n",
       "  (12): Linear(in_features=64, out_features=7, bias=True)\n",
       "  (13): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############params###############\n",
      "#############params###############\n"
     ]
    }
   ],
   "source": [
    "bs = 4096\n",
    "noise_multiplier = 7.0\n",
    "print(\"#############params###############\")\n",
    "param_list = []\n",
    "param_shapes = [p.shape for p in model.parameters()]\n",
    "for p in model.parameters():\n",
    "    param_list.append(p)\n",
    "print(\"#############params###############\")        \n",
    "\n",
    "\n",
    "optimizer_FTRLM = FTRLM( # noqa\n",
    "        params=param_list, \n",
    "        model_param_sizes=param_shapes, \n",
    "        lr=0.01, \n",
    "        momentum=0.9, \n",
    "        nesterov=True, \n",
    "        noise_std=(noise_multiplier / bs),\n",
    "        max_grad_norm=10.0,\n",
    "        seed=0,\n",
    "        efficient=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from absl import app\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "def convert_gaussian_renyi_to_dp(sigma, delta, verbose=True):\n",
    "    \"\"\"\n",
    "    Convert from RDP to DP for a Gaussian mechanism.\n",
    "    :param sigma: the algorithm guarantees (alpha, alpha/(2*sigma^2))-RDP\n",
    "    :param delta: target DP delta\n",
    "    :param verbose: whether to print message\n",
    "    :return: the DP epsilon\n",
    "    \"\"\"\n",
    "    alphas = np.arange(1, 200, 0.1)[1:]\n",
    "    epss = alphas / 2 / sigma**2 - (np.log(delta*(alphas - 1)) - alphas * np.log(1 - 1/alphas)) / (alphas - 1)\n",
    "    idx = np.nanargmin(epss)\n",
    "    if verbose and idx == len(alphas) - 1:\n",
    "        print('The best alpha is the last one. Consider increasing the range of alpha.')\n",
    "    eps = epss[idx]\n",
    "    return eps\n",
    "\n",
    "\n",
    "def get_total_sensitivity_sq_given_order(order):\n",
    "    \"\"\"\n",
    "    Get the squared sensitivity for a given order of batches.\n",
    "    Can be viewed as a general case for get_total_sensitivity_sq_same_order.\n",
    "    This function is not used in the privacy computation, as we operated in the case where the\n",
    "    data order is the same for every epoch.\n",
    "    :param order: a list representing the order of the batches, e.g. [0,1,2,1] means we use batch indexed with 0,1,2,1.\n",
    "                  -1 indicates virtual step.\n",
    "    :return: squared sensitivity, squared_sensitivity with respect to all every batch\n",
    "    \"\"\"\n",
    "    # get first layer as a list of counters\n",
    "    layer = [Counter({node: 1}) for node in order]\n",
    "\n",
    "    # sensitivity_sq[i] will record the total sensitivity wrt batch i\n",
    "    sensitivity_sq_all = [0] * (max(order) + 1)\n",
    "\n",
    "    # update sensitivity_sq with a given layer\n",
    "    def update_sensitivity_sq(current_layer):\n",
    "        for node in current_layer:\n",
    "            for ss in node:\n",
    "                if ss != -1:\n",
    "                    sensitivity_sq_all[ss] += node[ss] ** 2\n",
    "\n",
    "    update_sensitivity_sq(layer)  # get sensitivity for the first layer\n",
    "    while len(layer) > 1:\n",
    "        layer_new = []  # merge every two consecutive nodes to get the next layer\n",
    "        length = len(layer)\n",
    "        for i in range(0, length, 2):\n",
    "            if i + 1 < length:\n",
    "                layer_new.append(layer[i] + layer[i + 1])\n",
    "        layer = layer_new\n",
    "        update_sensitivity_sq(layer)\n",
    "    return max(sensitivity_sq_all), sensitivity_sq_all\n",
    "\n",
    "\n",
    "def get_total_sensitivity_sq_same_order(steps_per_epoch, epochs, extra_steps, mem_fn=None):\n",
    "    \"\"\"\n",
    "    Get the squared sensitivty for a tree where we fix the order of batches for all epochs.\n",
    "    :param steps_per_epoch: number of steps per epoch\n",
    "    :param epochs: number of epochs in the tree\n",
    "    :param extra_steps: number of virtual steps\n",
    "    :param mem_fn: if set, will write result to the file\n",
    "    :return: squared sensitivity, squared sensivity assuming no virtual steps,\n",
    "             squared sensitivity with respect to every batch\n",
    "    e.g. steps_per_epochs = 3 and epochs = 2, extra_steps = 2 means we have three batches b1, b2, b3,\n",
    "    and train w/ [b1, b2, b3, b1, b2, b3, +, +] where + means the extra steps.\n",
    "    We will enumerate through all nodes layer by layer in list \"layer\", and  compute the sensitivty\n",
    "    with respect to every node in \"sensitivity_sq\".\n",
    "    \"\"\"\n",
    "    # to record the result to save computation\n",
    "    mem = json.load(open(mem_fn)) if mem_fn else {}\n",
    "    key = f'{steps_per_epoch},{epochs},{extra_steps}'\n",
    "    key_no_extra = f'{steps_per_epoch},{epochs},{0}'\n",
    "    if key in mem and key_no_extra in mem:\n",
    "        return mem[key], mem[key_no_extra], None\n",
    "\n",
    "    # get first layer as a list of counters, the keys are batches (indexed with non-negative numbers), counts are\n",
    "    # number of times the batch appears in the node\n",
    "    layer = []\n",
    "    for _ in range(epochs):\n",
    "        layer += [Counter({ss: 1}) for ss in range(steps_per_epoch)]\n",
    "    layer += [Counter({-1: 1}) for _ in range(extra_steps)]  # extra steps denoted as -1\n",
    "\n",
    "    # sensitivity_sq[i] will record the total sensitivity wrt batch i\n",
    "    sensitivity_sq_all = [0] * steps_per_epoch\n",
    "    sensitivity_sq_all_no_extra = [0] * steps_per_epoch  # will also compute sensitivity without extra\n",
    "\n",
    "    # update sensitivity_sq with a given layer\n",
    "    def update_sensitivity_sq(current_layer):\n",
    "        for node in current_layer:\n",
    "            has_extra = -1 in node\n",
    "            for ss in node:\n",
    "                if ss != -1:\n",
    "                    sensitivity_sq_all[ss] += node[ss] ** 2\n",
    "                    if not has_extra:\n",
    "                        sensitivity_sq_all_no_extra[ss] += node[ss] ** 2\n",
    "\n",
    "    update_sensitivity_sq(layer)  # get sensitivity for the first layer\n",
    "    while len(layer) > 1:\n",
    "        layer_new = []  # merge every two consecutive nodes to get the next layer\n",
    "        length = len(layer)\n",
    "        for i in range(0, length, 2):\n",
    "            if i + 1 < length:\n",
    "                layer_new.append(layer[i] + layer[i + 1])\n",
    "        del layer\n",
    "        layer = layer_new\n",
    "        update_sensitivity_sq(layer)\n",
    "\n",
    "    # save to file\n",
    "    if mem_fn:\n",
    "        mem[key] = max(sensitivity_sq_all)\n",
    "        mem[key_no_extra] = max(sensitivity_sq_all_no_extra)\n",
    "        with open(mem_fn, 'w') as f:\n",
    "            json.dump(mem, f, indent=4)\n",
    "    return max(sensitivity_sq_all), max(sensitivity_sq_all_no_extra), sensitivity_sq_all\n",
    "\n",
    "\n",
    "def compute_epsilon_tree_restart_rdp_same_order_extra(num_batches: int, epochs_between_restarts: List[int],\n",
    "                                                      noise: float, tree_completion: bool = True,\n",
    "                                                      mem_fn: str = None):\n",
    "    \"\"\"\n",
    "    Compute the effective noise for DP-FTRL.\n",
    "    :param num_batches: number of batches per epoch\n",
    "    :param epochs_between_restarts: number of epochs between each restart, e.g. [2, 1] means epoch1, epoch2, restart, epoch3\n",
    "    :param noise: noise multiplier for each step\n",
    "    :param tree_completion: if true, use the tree completion trick which adds virtual steps to complete the binary tree\n",
    "    :param mem_fn: if set, will write result to the file\n",
    "    :return: the effective noise for DP-FTRL\n",
    "    \"\"\"\n",
    "    if noise < 1e-20:\n",
    "        return float('inf')\n",
    "\n",
    "    mem = {}  # to record result to avoid computing the same setting twice\n",
    "    sensitivity_sq = 0  # total sensitivity^2, which is the sum over all \"intervals\" between each restarting\n",
    "    for i, epochs in enumerate(epochs_between_restarts):\n",
    "        if epochs == 0:\n",
    "            continue\n",
    "        if tree_completion and i < len(epochs_between_restarts) - 1:\n",
    "            # compute number of virtual steps\n",
    "            extra_steps = 2 ** (num_batches * epochs - 1).bit_length() - num_batches * epochs\n",
    "        else:\n",
    "            extra_steps = 0\n",
    "        key = (num_batches, epochs, extra_steps)\n",
    "        mem[key] = mem.get(key,\n",
    "                           get_total_sensitivity_sq_same_order(num_batches, epochs, extra_steps, mem_fn)[0])\n",
    "        sensitivity_sq += mem[key]\n",
    "    effective_sigma = noise / np.sqrt(sensitivity_sq)\n",
    "    return effective_sigma\n",
    "\n",
    "\n",
    "def compute_epsilon_tree(num_batches: int, epochs_between_restarts: List[int], noise: float, delta: float,\n",
    "                         tree_completion: bool,\n",
    "                         verbose=True, mem_fn=None):\n",
    "    \"\"\"\n",
    "    Compute epsilon value for DP-FTRL.\n",
    "    :param num_batches: number of batches per epoch\n",
    "    :param epochs_between_restarts: number of epochs between each restart, e.g. [2, 1] means epoch1, epoch2, restart, epoch3\n",
    "    :param noise: noise multiplier for each step\n",
    "    :param delta: target DP delta\n",
    "    :param tree_completion: if true, use the tree completion trick which adds virtual steps to complete the binary tree\n",
    "    :param verbose: whether to print message\n",
    "    :param mem_fn: if set, will write result to the file\n",
    "    :return: the DP epsilon for DP-FTRL\n",
    "    \"\"\"\n",
    "\n",
    "    if noise < 1e-20:\n",
    "        return float('inf')\n",
    "\n",
    "    effective_sigma = compute_epsilon_tree_restart_rdp_same_order_extra(num_batches, epochs_between_restarts, noise,\n",
    "                                                                        tree_completion, mem_fn)\n",
    "    eps = convert_gaussian_renyi_to_dp(effective_sigma, delta, verbose)\n",
    "    return eps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py in /raid/ganesh/racha_suraj/miniconda3/envs/dpo/lib/python3.12/site-packages (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install absl-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=400000, batch=4096, epochs=100 with restarting every 1 epochs noise=1000.0, tree_completion=True gives (0.10, 1e-05)-DP\n"
     ]
    }
   ],
   "source": [
    "n = 400000\n",
    "delta = 1e-5\n",
    "batch = 4096\n",
    "epochs = 100\n",
    "restart_every = 1\n",
    "noise = 1000.0\n",
    "tree_completion = True\n",
    "\n",
    "num_batches = n // batch\n",
    "epochs_between_restarts = [restart_every] * (epochs // restart_every)\n",
    "\n",
    "eps = compute_epsilon_tree(num_batches, epochs_between_restarts, noise, delta, tree_completion)\n",
    "\n",
    "print(f'n={n}, batch={batch}, epochs={epochs} with restarting every {restart_every} epochs', f'noise={noise}, tree_completion={tree_completion}',f'gives ({eps:.2f}, {delta})-DP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2024 15:59:46:ERROR:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mponkshekaustubh11\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/raid/ganesh/racha_suraj/SF_test/Power-Mechanism-new/Forrest_Cover_Type/wandb/run-20240518_155948-b92v23ol</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ponkshekaustubh11/covtype/runs/b92v23ol' target=\"_blank\">FTRL</a></strong> to <a href='https://wandb.ai/ponkshekaustubh11/covtype' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ponkshekaustubh11/covtype' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covtype</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ponkshekaustubh11/covtype/runs/b92v23ol' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covtype/runs/b92v23ol</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"covtype\",name=\"FTRL\")\n",
    "num_epochs = 100\n",
    "max_steps = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb(model, train_loader, criterion, optimizer_FTRLM, num_epochs=num_epochs,device=torch.device('cpu'),test_loader = test_loader,max_steps = max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
