{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import dcMinMaxFunctions as dc\n",
        "# import dcor\n",
        "from scipy.misc import derivative\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from scipy import stats\n",
        "import wandb\n",
        "from cov_help import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(54, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 7),\n",
        "            nn.Softmax(dim=1)\n",
        "\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45383\n"
          ]
        }
      ],
      "source": [
        "#find number of parameters in model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Horizontal_Distance_To_Hydrology</th>\n",
              "      <th>Vertical_Distance_To_Hydrology</th>\n",
              "      <th>Horizontal_Distance_To_Roadways</th>\n",
              "      <th>Hillshade_9am</th>\n",
              "      <th>Hillshade_Noon</th>\n",
              "      <th>Hillshade_3pm</th>\n",
              "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
              "      <th>...</th>\n",
              "      <th>Soil_Type32</th>\n",
              "      <th>Soil_Type33</th>\n",
              "      <th>Soil_Type34</th>\n",
              "      <th>Soil_Type35</th>\n",
              "      <th>Soil_Type36</th>\n",
              "      <th>Soil_Type37</th>\n",
              "      <th>Soil_Type38</th>\n",
              "      <th>Soil_Type39</th>\n",
              "      <th>Soil_Type40</th>\n",
              "      <th>Cover_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596</td>\n",
              "      <td>51</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "      <td>221</td>\n",
              "      <td>232</td>\n",
              "      <td>148</td>\n",
              "      <td>6279</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5180</td>\n",
              "      <td>112</td>\n",
              "      <td>4</td>\n",
              "      <td>424</td>\n",
              "      <td>-12</td>\n",
              "      <td>780</td>\n",
              "      <td>440</td>\n",
              "      <td>470</td>\n",
              "      <td>302</td>\n",
              "      <td>12450</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804</td>\n",
              "      <td>139</td>\n",
              "      <td>9</td>\n",
              "      <td>268</td>\n",
              "      <td>65</td>\n",
              "      <td>3180</td>\n",
              "      <td>234</td>\n",
              "      <td>238</td>\n",
              "      <td>135</td>\n",
              "      <td>6121</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785</td>\n",
              "      <td>155</td>\n",
              "      <td>18</td>\n",
              "      <td>242</td>\n",
              "      <td>118</td>\n",
              "      <td>3090</td>\n",
              "      <td>238</td>\n",
              "      <td>238</td>\n",
              "      <td>122</td>\n",
              "      <td>6211</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>391</td>\n",
              "      <td>220</td>\n",
              "      <td>234</td>\n",
              "      <td>150</td>\n",
              "      <td>6172</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581007</th>\n",
              "      <td>2396</td>\n",
              "      <td>153</td>\n",
              "      <td>20</td>\n",
              "      <td>85</td>\n",
              "      <td>17</td>\n",
              "      <td>108</td>\n",
              "      <td>240</td>\n",
              "      <td>237</td>\n",
              "      <td>118</td>\n",
              "      <td>837</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581008</th>\n",
              "      <td>2391</td>\n",
              "      <td>152</td>\n",
              "      <td>19</td>\n",
              "      <td>67</td>\n",
              "      <td>12</td>\n",
              "      <td>95</td>\n",
              "      <td>240</td>\n",
              "      <td>237</td>\n",
              "      <td>119</td>\n",
              "      <td>845</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581009</th>\n",
              "      <td>2386</td>\n",
              "      <td>159</td>\n",
              "      <td>17</td>\n",
              "      <td>60</td>\n",
              "      <td>7</td>\n",
              "      <td>90</td>\n",
              "      <td>236</td>\n",
              "      <td>241</td>\n",
              "      <td>130</td>\n",
              "      <td>854</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581010</th>\n",
              "      <td>2384</td>\n",
              "      <td>170</td>\n",
              "      <td>15</td>\n",
              "      <td>60</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>230</td>\n",
              "      <td>245</td>\n",
              "      <td>143</td>\n",
              "      <td>864</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581011</th>\n",
              "      <td>2383</td>\n",
              "      <td>165</td>\n",
              "      <td>13</td>\n",
              "      <td>60</td>\n",
              "      <td>4</td>\n",
              "      <td>67</td>\n",
              "      <td>231</td>\n",
              "      <td>244</td>\n",
              "      <td>141</td>\n",
              "      <td>875</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>581012 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
              "0            2596      51      3                               258   \n",
              "1            5180     112      4                               424   \n",
              "2            2804     139      9                               268   \n",
              "3            2785     155     18                               242   \n",
              "4            2595      45      2                               153   \n",
              "...           ...     ...    ...                               ...   \n",
              "581007       2396     153     20                                85   \n",
              "581008       2391     152     19                                67   \n",
              "581009       2386     159     17                                60   \n",
              "581010       2384     170     15                                60   \n",
              "581011       2383     165     13                                60   \n",
              "\n",
              "        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
              "0                                    0                              510   \n",
              "1                                  -12                              780   \n",
              "2                                   65                             3180   \n",
              "3                                  118                             3090   \n",
              "4                                   -1                              391   \n",
              "...                                ...                              ...   \n",
              "581007                              17                              108   \n",
              "581008                              12                               95   \n",
              "581009                               7                               90   \n",
              "581010                               5                               90   \n",
              "581011                               4                               67   \n",
              "\n",
              "        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
              "0                 221             232            148   \n",
              "1                 440             470            302   \n",
              "2                 234             238            135   \n",
              "3                 238             238            122   \n",
              "4                 220             234            150   \n",
              "...               ...             ...            ...   \n",
              "581007            240             237            118   \n",
              "581008            240             237            119   \n",
              "581009            236             241            130   \n",
              "581010            230             245            143   \n",
              "581011            231             244            141   \n",
              "\n",
              "        Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  \\\n",
              "0                                     6279  ...            0            0   \n",
              "1                                    12450  ...            0            0   \n",
              "2                                     6121  ...            0            0   \n",
              "3                                     6211  ...            0            0   \n",
              "4                                     6172  ...            0            0   \n",
              "...                                    ...  ...          ...          ...   \n",
              "581007                                 837  ...            0            0   \n",
              "581008                                 845  ...            0            0   \n",
              "581009                                 854  ...            0            0   \n",
              "581010                                 864  ...            0            0   \n",
              "581011                                 875  ...            0            0   \n",
              "\n",
              "        Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
              "0                 0            0            0            0            0   \n",
              "1                 0            0            0            0            0   \n",
              "2                 0            0            0            0            0   \n",
              "3                 0            0            0            0            0   \n",
              "4                 0            0            0            0            0   \n",
              "...             ...          ...          ...          ...          ...   \n",
              "581007            0            0            0            0            0   \n",
              "581008            0            0            0            0            0   \n",
              "581009            0            0            0            0            0   \n",
              "581010            0            0            0            0            0   \n",
              "581011            0            0            0            0            0   \n",
              "\n",
              "        Soil_Type39  Soil_Type40  Cover_Type  \n",
              "0                 0            0           5  \n",
              "1                 0            0          10  \n",
              "2                 0            0           2  \n",
              "3                 0            0           2  \n",
              "4                 0            0           5  \n",
              "...             ...          ...         ...  \n",
              "581007            0            0           3  \n",
              "581008            0            0           3  \n",
              "581009            0            0           3  \n",
              "581010            0            0           3  \n",
              "581011            0            0           3  \n",
              "\n",
              "[581012 rows x 55 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.read_csv(\"data/covtype.csv\")\n",
        "# multiply second row by 2\n",
        "df.iloc[1] = df.iloc[1]*2\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(\"Cover_Type\",axis=1).values\n",
        "Y= df[\"Cover_Type\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    x_normed = 1*x / x.max(0, keepdim=True)[0]\n",
        "    return x_normed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = torch.Tensor(X)\n",
        "Y = torch.Tensor(Y)\n",
        "X = normalize(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_dist = torch.cdist(X, X).max()\n",
        "# print(max_dist)\n",
        "# for i in range(len(X)):\n",
        "#     if(i%1000==0):\n",
        "#         print(i)\n",
        "#         print\n",
        "#     for j in range(i,len(X)):\n",
        "#         dist = torch.dist(X[i],X[j])\n",
        "#         if(dist>max_dist):\n",
        "#             max_dist = dist\n",
        "            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Write code to convert Y from 1,2,3,4,5,6,7 to 0,1,2,3,4,5,6\n",
        "Y = Y-1\n",
        "Y = Y.type(torch.LongTensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X[::10]\n",
        "Y= Y[::10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9416)\n"
          ]
        }
      ],
      "source": [
        "data_path = \"data/covtype.csv\"\n",
        "norm=1\n",
        "X,Y = cov_data_loader(data_path,norm=norm)\n",
        "max_dist = torch.cdist(X, X).max()\n",
        "print(max_dist) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4, 4, 4, 4, 0, 4, 4, 1, 0, 1])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSikHVAlO3MY"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "#write a code to choose evenly sampled subset of X and Y so that only 10% data remains\n",
        "# X_train = X_train[::10]\n",
        "# Y_train = Y_train[::10]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating Data Densities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gaussian Kernel Density Estimation & Derivative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gau_ker(u):\n",
        "    return torch.pow(2*torch.tensor(torch.pi),u.shape[1]/(-2))*torch.exp(torch.bmm(u.view(u.shape[0], 1, u.shape[1]), u.view(u.shape[0],  u.shape[1],1))/(-2)).cuda()\n",
        "\n",
        "\n",
        "def py_kde(x,X_t,h):\n",
        "    norm = X_t.shape[0]*(h**x.shape[1])\n",
        "    prob = torch.zeros(x.shape[0]).cuda()\n",
        "    for i in range(len(X_t)):\n",
        "        prob+= (torch.squeeze(gau_ker((x - X_t[i])/h))/norm).cuda()\n",
        "    return(prob)\n",
        "\n",
        "\n",
        "def py_kde_der(p_x,x):\n",
        "    # x.requires_grad = True\n",
        "    # p_x = py_kde(x,X_t,h)\n",
        "    return (torch.autograd.grad(p_x,x,torch.ones_like(p_x),allow_unused=True,create_graph=True)[0]).cuda()\n",
        "\n",
        "\n",
        "def gau_ker_der(X,h):\n",
        "    N= X.shape[0]\n",
        "    d = X.shape[1]\n",
        "    grad = torch.zeros(X.shape)\n",
        "    for n in range(N):\n",
        "        for i in range(d):\n",
        "            for j in range(N):\n",
        "                grad[n][i]+= torch.exp(-1*torch.dot((X[n]-X[j]),(X[n]-X[j]))/(2*h*h))*(X[n][i] -X[j][i]) /(N*(h**(d+2))*((2*math.pi)**(d/2)))\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Confidence Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def CI_KDE(p_x,n,h,d,alpha):\n",
        "    return( stats.norm.ppf(1-alpha/2)*torch.sqrt(p_x/((2**d)*math.sqrt(torch.pi**d)*n*h**(d))).cuda() )\n",
        "\n",
        "def CI_KDE_der(p_x_der,p_x,n,h,d,alpha):\n",
        "    return( p_x_der*stats.norm.ppf(1-alpha/2)*torch.sqrt(1/(p_x.unsqueeze(dim=1)*(2**d)*math.sqrt(torch.pi**d)*n*h**(d))).cuda() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = len(X)\n",
        "d = X.shape[1]\n",
        "alpha=0.01\n",
        "h=n**(-1./(d+4))\n",
        "x = X[0:1000].detach().cuda()\n",
        "x.requires_grad = True\n",
        "f = py_kde(x,x,h)\n",
        "f_der = py_kde_der(f,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8276729740613151"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 4.2023e-02,  3.3746e-02,  4.1772e-03,  ...,  1.4568e-05,\n",
              "          1.1455e-05,  4.8000e-06],\n",
              "        [ 3.7382e-02, -1.3021e-02,  3.8616e-03,  ...,  1.4643e-05,\n",
              "          1.1495e-05,  4.8310e-06],\n",
              "        [ 7.1138e-02,  2.7528e-02,  2.3118e-03,  ...,  1.4539e-05,\n",
              "          1.1465e-05,  4.7379e-06],\n",
              "        ...,\n",
              "        [-1.8066e-01,  4.4793e-02, -1.0829e-03,  ...,  1.4479e-05,\n",
              "         -3.0024e-04,  4.3854e-06],\n",
              "        [-9.7191e-02,  1.3910e-02, -2.3581e-03,  ...,  1.4791e-05,\n",
              "          1.1736e-05,  4.6439e-06],\n",
              "        [-2.1025e-02,  3.1352e-02, -2.6411e-03,  ...,  1.4326e-05,\n",
              "          1.1524e-05,  4.3846e-06]], device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_der/f.unsqueeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "ci = CI_KDE(f,n,h,d,alpha)\n",
        "ci_der = CI_KDE_der(f_der,f,n,h,d,alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.H_net1 = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(256, 54*54).cuda()\n",
        "        )\n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=2)\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            H12 = self.H_net1(z)\n",
        "            H12= H12.reshape(z.shape[0],d,d)\n",
        "            x12 = torch.matmul(z,H12)\n",
        "            return(x12)\n",
        "    \n",
        "        \n",
        "        def batch_jacobian(func, z, create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x.cuda()\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            H = self.H_net1(z).cuda()\n",
        "            H = H.reshape(bs,d,d)\n",
        "            \n",
        "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
        "            z = torch.matmul(z,H).cuda()\n",
        "          \n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "            print(J_int)\n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).cuda()\n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-14.0633, -14.0712, -14.0702], device='cuda:0', grad_fn=<NegBackward0>)\n",
            "tensor([-13.8768, -13.8846, -13.9041], device='cuda:0', grad_fn=<NegBackward0>)\n",
            "tensor([-14.0622, -14.0588, -14.0439], device='cuda:0', grad_fn=<NegBackward0>)\n",
            "tensor([-14.2419, -14.2583, -14.2708], device='cuda:0', grad_fn=<NegBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.3234, -0.7693, -0.8887, -0.2414, -0.1618, -0.6293,  0.0275,  0.1875,\n",
              "          0.4580,  0.4194, -0.4029,  0.3065,  0.9666, -0.2815, -0.5516, -1.1587,\n",
              "          0.6399, -0.5782, -0.3469,  0.2795, -0.6986,  0.9810, -2.0371, -0.3194,\n",
              "         -0.6768, -0.0958, -0.6037,  2.1016,  0.3830, -0.4112,  0.5383,  1.2991,\n",
              "         -0.6947, -0.1659,  0.0726, -0.1052, -1.5091,  0.8768, -0.6838,  0.5625,\n",
              "          0.5225,  0.7852,  1.3750, -0.3908,  0.4139,  0.7174, -0.3678, -0.3707,\n",
              "         -0.0808, -0.5189,  0.4334, -0.2020, -0.9053, -0.6403],\n",
              "        [-0.3110, -0.7791, -0.8755, -0.2442, -0.1414, -0.6387,  0.0310,  0.1878,\n",
              "          0.4460,  0.4113, -0.4104,  0.3062,  0.9773, -0.2675, -0.5538, -1.1700,\n",
              "          0.6277, -0.5732, -0.3426,  0.2462, -0.6812,  0.9584, -2.0119, -0.3063,\n",
              "         -0.6908, -0.0949, -0.5891,  2.0933,  0.3686, -0.4315,  0.5448,  1.2842,\n",
              "         -0.6983, -0.1528,  0.0594, -0.1210, -1.5011,  0.8799, -0.6839,  0.5717,\n",
              "          0.5131,  0.7796,  1.3724, -0.3795,  0.3972,  0.7090, -0.3739, -0.3864,\n",
              "         -0.0998, -0.5297,  0.4155, -0.2128, -0.8989, -0.6248],\n",
              "        [-0.3480, -0.7574, -0.8414, -0.2591, -0.1564, -0.6288,  0.0072,  0.1678,\n",
              "          0.4424,  0.4161, -0.4092,  0.3192,  0.9407, -0.2753, -0.5292, -1.1581,\n",
              "          0.6214, -0.6192, -0.3596,  0.2753, -0.6804,  0.9486, -1.9949, -0.3294,\n",
              "         -0.6486, -0.0828, -0.5867,  2.0829,  0.3230, -0.4152,  0.5219,  1.2448,\n",
              "         -0.6790, -0.1594,  0.0752, -0.1215, -1.4958,  0.9056, -0.6560,  0.5298,\n",
              "          0.5241,  0.8048,  1.3607, -0.3666,  0.3922,  0.7032, -0.3822, -0.4311,\n",
              "         -0.0800, -0.5361,  0.4241, -0.1482, -0.9128, -0.6524]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net(4).to(torch.device(\"cuda\"))\n",
        "x = X[0:3]\n",
        "net(x)\n",
        "net.loss_reg\n",
        "\n",
        "# loss = net.loss_reg.sum()\n",
        "# loss.backward(retain_graph=True)\n",
        "# x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net2(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net2, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.H_net1 = nn.Sequential(\n",
        "            nn.Linear(18, 128),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(256, 18*18).cuda()\n",
        "        )\n",
        "        self.H_net2 = nn.Sequential(\n",
        "            nn.Linear(18, 128),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(256, 18*18).cuda()\n",
        "        )\n",
        "        self.H_net3 = nn.Sequential(\n",
        "            nn.Linear(18, 128),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(256, 18*18).cuda()\n",
        "        )\n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=2)\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            H11 = self.H_net1(z[:,:,0:18])\n",
        "            H12 = self.H_net2(z[:,:,18:36])\n",
        "            H13 = self.H_net3(z[:,:,36:54])\n",
        "            H11= H11.reshape(z.shape[0],18,18)\n",
        "            H12= H12.reshape(z.shape[0],18,18)\n",
        "            H13= H13.reshape(z.shape[0],18,18)\n",
        "            x11 = torch.matmul(z[:,:,0:18],H11)\n",
        "            x12 = torch.matmul(z[:,:,18:36],H12)\n",
        "            x13 = torch.matmul(z[:,:,36:54],H13)\n",
        "        \n",
        "            x = torch.cat((x11,x12,x13),dim=2)\n",
        "            \n",
        "            return(x)\n",
        "    \n",
        "        \n",
        "        def batch_jacobian(func, z, create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x.cuda()\n",
        "        z1 = z[:,:,0:18]\n",
        "        z2 = z[:,:,18:36]\n",
        "        z3 = z[:,:,36:54]\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            \n",
        "            H1 = self.H_net1(z1).cuda()\n",
        "            H2 = self.H_net2(z2).cuda()\n",
        "            H3 = self.H_net3(z3).cuda()\n",
        "            H1 = H1.reshape(bs,18,18)\n",
        "            H2 = H2.reshape(bs,18,18)\n",
        "            H3 = H3.reshape(bs,18,18)\n",
        "            \n",
        "\n",
        "            \n",
        "            \n",
        "            # H = H.reshape(bs,d,d)\n",
        "            \n",
        "            # J1 = batch_jacobian(H_mul1, z1, create_graph=True)\n",
        "            # J2 = batch_jacobian(H_mul2, z2, create_graph=True)\n",
        "            # J3 = batch_jacobian(H_mul3, z3, create_graph=True)\n",
        "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
        "           \n",
        "\n",
        "            z1 = torch.matmul(z1,H1).cuda()\n",
        "            z2 = torch.matmul(z2,H2).cuda()\n",
        "            z3 = torch.matmul(z3,H3).cuda()\n",
        "            z = torch.cat((z1,z2,z3),dim=2).cuda()\n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).cuda()\n",
        "           \n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y.squeeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "cusolver error: CUSOLVER_STATUS_INTERNAL_ERROR, when calling `cusolverDnCreate(handle)`. If you keep seeing this error, you may use `torch.backends.cuda.preferred_linalg_library()` to try linear algebra operators with other supported backends. See https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.preferred_linalg_library",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m Net2(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mloss_reg\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      6\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[23], line 102\u001b[0m, in \u001b[0;36mNet2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z1,z2,z3),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    101\u001b[0m     J_int \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mabs(torch\u001b[38;5;241m.\u001b[39mdet(J)))\n\u001b[0;32m--> 102\u001b[0m     loss_reg \u001b[38;5;241m=\u001b[39m loss_reg \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJ_int\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_reg \u001b[38;5;241m=\u001b[39m loss_reg\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m=\u001b[39m z\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cusolver error: CUSOLVER_STATUS_INTERNAL_ERROR, when calling `cusolverDnCreate(handle)`. If you keep seeing this error, you may use `torch.backends.cuda.preferred_linalg_library()` to try linear algebra operators with other supported backends. See https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.preferred_linalg_library"
          ]
        }
      ],
      "source": [
        "net = Net2(4).to(torch.device(\"cuda\"))\n",
        "x = X[0:3]\n",
        "net(x)\n",
        "\n",
        "loss = net.loss_reg.sum()\n",
        "loss.backward(retain_graph=True)\n",
        "x.grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net5(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net5, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.J_Int =0\n",
        "\n",
        "        self.H_net1 = nn.Sequential(\n",
        "\n",
        "            nn.Linear(54, 128),\n",
        "            nn.LeakyReLU(10),\n",
        "            # nn.Sigmoid(),\n",
        "            nn.Linear(128, 64),\n",
        "            # nn.LeakyReLU(10),\n",
        "            #  nn.Linear(256, 128),\n",
        "            nn.LeakyReLU(5),\n",
        "            # nn.Sigmoid(),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(128, 64),\n",
        "            # nn.LeakyReLU(5),\n",
        "\n",
        "            nn.Linear(64, 54).cuda()\n",
        "        )\n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=2)\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            y = self.H_net1(z)\n",
        "            # y = torch.norm(z,dim =2).unsqueeze(1)*y\n",
        "            \n",
        "            return(y)\n",
        "\n",
        "           \n",
        "    \n",
        "        \n",
        "        def batch_jacobian(func, z, retain_graph = False,create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x.cuda()\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            y = self.H_net1(z).cuda()\n",
        "           \n",
        "            # y = torch.norm(x.cuda(),dim =2).unsqueeze(1)*y\n",
        "          \n",
        "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
        "            # print(torch.det(J)) \n",
        "           \n",
        "            # J_int =torch.det(J)\n",
        "\n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "          \n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,retain_graph= True)[0]).cuda()\n",
        "            z = y\n",
        "        \n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0.]], device='cuda:0')"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net5(1).to(torch.device(\"cuda\"))\n",
        "net(X[0:4])\n",
        "\n",
        "net.loss_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 54])"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X[0:2].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "# make_dot was moved to https://github.com/szagoruyko/pytorchviz\n",
        "from torchviz import make_dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Digraph.gv.pdf'"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error: no \"view\" rule for type \"application/pdf\" passed its test case\n",
            "       (for more information, add \"--debug=1\" on the command line)\n"
          ]
        }
      ],
      "source": [
        "make_dot(net.loss_reg).view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net3(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net3, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.k = 20\n",
        "        self.H_net1 = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.LeakyReLU(10),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(64, 54*2*self.k).cuda()\n",
        "        )\n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=2)\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            h_c_m = self.H_net1(z)\n",
        "            h_1_m = h_c_m[:,:,0:self.k*d]\n",
        "            h_2_m = h_c_m[:,:,self.k*d:]\n",
        "            h_1_m = h_1_m.reshape(z.shape[0],self.k,d)\n",
        "            h_2_m = h_2_m.reshape(z.shape[0],self.k,d)\n",
        "            h_2_t_m = torch.transpose(h_2_m, 1, 2)\n",
        "            H_m = torch.matmul(h_2_t_m,h_1_m)\n",
        "            z_m = torch.matmul(z,H_m)\n",
        "            return(z_m)\n",
        "\n",
        "           \n",
        "    \n",
        "        \n",
        "        def batch_jacobian(func, z, create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x.cuda()\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            h_c = self.H_net1(z).cuda()\n",
        "            h_1 = h_c[:,:,0:self.k*d]\n",
        "            h_2 = h_c[:,:,self.k*d:]\n",
        "           \n",
        "\n",
        "            h_1 = h_1.reshape(bs,self.k,d)\n",
        "            h_2 = h_2.reshape(bs,self.k,d)\n",
        "            h_2_t = torch.transpose(h_2, 1, 2)\n",
        "       \n",
        "            H = torch.matmul(h_2_t,h_1).cuda()\n",
        "          \n",
        "          \n",
        "            H = H.reshape(bs,d,d)\n",
        "            \n",
        "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
        "\n",
        "            z = torch.matmul(z,H).cuda()\n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "            print(J_int)\n",
        "           \n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).cuda()\n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([inf, inf, inf], device='cuda:0', grad_fn=<NegBackward0>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan],\n",
              "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "         nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net3(1).to(torch.device(\"cuda\"))\n",
        "net(X[0:3])\n",
        "\n",
        "net.loss_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(LeakyReLU(negative_slope=1.1),)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.LeakyReLU(1.1),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net4(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net4, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.H_net2 = nn.Sequential(\n",
        "            nn.Linear(54, 128).cuda(),\n",
        "            nn.LeakyReLU(3),\n",
        "            # nn.Sigmoid(),\n",
        "        \n",
        "            nn.Linear(128, 256),\n",
        "            nn.LeakyReLU(3),\n",
        "            # nn.Sigmoid(),\n",
        "            nn.Linear(256, 54).cuda()\n",
        "        )\n",
        "\n",
        "\n",
        "        \n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(54, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=2)\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            x12 = self.H_net2(z)\n",
        "\n",
        "            x12 = torch.norm(z,dim=2)*x12\n",
        "            \n",
        "            # x12 = x12*z\n",
        "            return(x12)\n",
        "\n",
        "\n",
        "        \n",
        "        def batch_jacobian(func, z, create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "          \n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        x = x.cuda()\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            y = self.H_net2(z).cuda()\n",
        "            print(y.shape)\n",
        "            print(torch.norm(z,dim=2).shape)\n",
        "            \n",
        "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
        "            # print(J)\n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "            z = torch.norm(z,dim=2)*y\n",
        "            print(torch.det(J))\n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).cuda()\n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 54])\n",
            "torch.Size([2, 1])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m Net4(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m net\u001b[38;5;241m.\u001b[39mloss_reg\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[32], line 66\u001b[0m, in \u001b[0;36mNet4.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnorm(z,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 66\u001b[0m J \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH_mul\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# print(J)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m J_int \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mabs(torch\u001b[38;5;241m.\u001b[39mdet(J)))\n",
            "Cell \u001b[0;32mIn[32], line 50\u001b[0m, in \u001b[0;36mNet4.forward.<locals>.batch_jacobian\u001b[0;34m(func, z, create_graph)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_func_sum\u001b[39m(z):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(z)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_func_sum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
          ]
        }
      ],
      "source": [
        "net = Net4(1).to(torch.device(\"cuda\"))\n",
        "net(X[0:2])\n",
        "\n",
        "net.loss_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m----> 2\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([[1.0,0,0],[2,0,0]])\n",
        "b = torch.norm(a,dim=1)*a\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.]])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.norm(torch.tensor([[[1.0,0,0],[2.0,0,0]]]),dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCPV_zl0O3Mc"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# criterion = nn.BCELoss(reduction= 'none')\n",
        "def my_loss(y_pred,y_train,reg_loss):\n",
        "    loss = criterion(y_pred,y_train) +reg_loss\n",
        "    return loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "net_5 = Net(1)\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "#loss function for multiclass classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_priv(net,trainloader,optimizer,epochs,h,rate=10,device= torch.device('cpu'),print_cond = True,only_reg_flag=0):\n",
        "    # scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    net = net.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        # scheduler.step()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            \n",
        "            inputs = data[0].to(device)\n",
        "            inputs.requires_grad = True\n",
        "            labels = data[1].to(device)\n",
        "            f = py_kde(inputs,inputs,h)\n",
        "            f_der = py_kde_der(f,inputs)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            if(only_reg_flag):\n",
        "                loss = torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum()\n",
        "                \n",
        "            else:\n",
        "                loss = criterion(torch.squeeze(outputs),torch.squeeze(labels)) + torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "          \n",
        "\n",
        "      \n",
        "            # print statistics\n",
        "            # print(loss.sum().shape)\n",
        "            running_loss += loss.sum().detach().cpu()\n",
        "            # if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            # if((i+1)%10==0):\n",
        "            if(True):\n",
        "                if(print_cond):\n",
        "                    print('%d loss: %.10f' %\n",
        "                            (epoch + 1, running_loss /(rate*10*trainloader.batch_size)))\n",
        "                    running_loss = 0.0\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=3000,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[117], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m net_5 \u001b[38;5;241m=\u001b[39m Net5(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(net_5\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_model_priv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_5\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainloader_priv\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.82\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43monly_reg_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Code3/Forrest Cover Type/cov_help.py:423\u001b[0m, in \u001b[0;36mtrain_model_priv\u001b[0;34m(net, trainloader, optimizer, epochs, h, rate, device, print_cond, only_reg_flag, lr_schedular)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    422\u001b[0m loss_reg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(f_der\u001b[38;5;241m/\u001b[39mf\u001b[38;5;241m.\u001b[39mview(f\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m net\u001b[38;5;241m.\u001b[39mloss_reg,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m--> 423\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_reg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mloss_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# print(loss.sum().shape)\u001b[39;00m\n\u001b[1;32m    427\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ],
      "source": [
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=1000,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "net_5 = Net5(1)\n",
        "optim = torch.optim.Adam(net_5.parameters(),lr=0.001)\n",
        "train_model_priv(net_5,trainloader_priv,optim,30,h=0.82,rate=1,device=torch.device('cuda'),only_reg_flag=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# # criterion = nn.BCELoss(reduction= 'none')\n",
        "# def my_loss(y_pred,y_train,reg_loss):\n",
        "#     loss = criterion(y_pred,y_train) +reg_loss\n",
        "#     return loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "net_5 = Net5(1)\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "#loss function for multiclass classification\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch number: 0   Iteration number: 0  LR : 1e-05  Loss: tensor(0.8930, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8930, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 400  LR : 1e-05  Loss: tensor(0.4961, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4961, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 800  LR : 1e-05  Loss: tensor(0.7659, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7659, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 1200  LR : 1e-05  Loss: tensor(1.0060, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.0060, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 1600  LR : 1e-05  Loss: tensor(0.8932, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8932, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 2000  LR : 1e-05  Loss: tensor(0.9420, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9420, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 2400  LR : 1e-05  Loss: tensor(1.0675, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.0675, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 2800  LR : 1e-05  Loss: tensor(1.0936, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.0936, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 3200  LR : 1e-05  Loss: tensor(1.0489, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.0489, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 3600  LR : 1e-05  Loss: tensor(1.0150, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.0150, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 4000  LR : 1e-05  Loss: tensor(0.9768, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9768, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 4400  LR : 1e-05  Loss: tensor(0.9324, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9324, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 4800  LR : 1e-05  Loss: tensor(0.9024, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9024, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 5200  LR : 1e-05  Loss: tensor(0.8637, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8637, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 5600  LR : 1e-05  Loss: tensor(0.7984, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7984, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 6000  LR : 1e-05  Loss: tensor(0.7491, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7491, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 6400  LR : 1e-05  Loss: tensor(0.8221, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8221, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 6800  LR : 1e-05  Loss: tensor(0.9328, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9328, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 7200  LR : 1e-05  Loss: tensor(0.9732, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9732, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 7600  LR : 1e-05  Loss: tensor(0.9929, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9929, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 8000  LR : 1e-05  Loss: tensor(0.9771, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9771, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 8400  LR : 1e-05  Loss: tensor(0.9495, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9495, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 8800  LR : 1e-05  Loss: tensor(0.9082, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9082, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 9200  LR : 1e-05  Loss: tensor(0.8532, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8532, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 9600  LR : 1e-05  Loss: tensor(0.8453, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8453, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 10000  LR : 1e-05  Loss: tensor(0.8180, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8180, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 10400  LR : 1e-05  Loss: tensor(0.8369, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8369, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 10800  LR : 1e-05  Loss: tensor(0.8083, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8083, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 11200  LR : 1e-05  Loss: tensor(0.7841, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7841, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 11600  LR : 1e-05  Loss: tensor(0.8228, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8228, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 12000  LR : 1e-05  Loss: tensor(0.8187, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8187, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 12400  LR : 1e-05  Loss: tensor(0.7720, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7720, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 12800  LR : 1e-05  Loss: tensor(0.7336, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7336, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 13200  LR : 1e-05  Loss: tensor(0.7026, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7026, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 13600  LR : 1e-05  Loss: tensor(0.6979, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6979, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 14000  LR : 1e-05  Loss: tensor(0.7088, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7088, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 14400  LR : 1e-05  Loss: tensor(0.7241, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7241, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 14800  LR : 1e-05  Loss: tensor(0.7324, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7324, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 15200  LR : 1e-05  Loss: tensor(0.7525, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7525, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 15600  LR : 1e-05  Loss: tensor(0.7581, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7581, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 16000  LR : 1e-05  Loss: tensor(0.7617, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7617, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 16400  LR : 1e-05  Loss: tensor(0.7835, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7835, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 16800  LR : 1e-05  Loss: tensor(0.8515, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8515, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 17200  LR : 1e-05  Loss: tensor(0.8904, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8904, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 17600  LR : 1e-05  Loss: tensor(0.8939, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8939, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 18000  LR : 1e-05  Loss: tensor(0.8472, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8472, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 18400  LR : 1e-05  Loss: tensor(0.8152, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8152, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 18800  LR : 1e-05  Loss: tensor(0.8180, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8180, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 19200  LR : 1e-05  Loss: tensor(0.7760, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7760, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 19600  LR : 1e-05  Loss: tensor(0.7754, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7754, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 20000  LR : 1e-05  Loss: tensor(0.8144, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8144, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 20400  LR : 1e-05  Loss: tensor(0.8609, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8609, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 20800  LR : 1e-05  Loss: tensor(0.9194, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.9194, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 21200  LR : 1e-05  Loss: tensor(0.8912, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.8912, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 21600  LR : 1e-05  Loss: tensor(0.7402, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.7402, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 22000  LR : 1e-05  Loss: tensor(0.6038, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6038, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 22400  LR : 1e-05  Loss: tensor(0.5459, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.5459, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 22800  LR : 1e-05  Loss: tensor(0.4835, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4835, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 23200  LR : 1e-05  Loss: tensor(0.4625, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4625, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 23600  LR : 1e-05  Loss: tensor(0.4444, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4444, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 24000  LR : 1e-05  Loss: tensor(0.4349, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4349, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 24400  LR : 1e-05  Loss: tensor(0.4373, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4373, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 24800  LR : 1e-05  Loss: tensor(0.4854, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4854, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 25200  LR : 1e-05  Loss: tensor(0.5219, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.5219, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 25600  LR : 1e-05  Loss: tensor(0.5552, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.5552, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 26000  LR : 1e-05  Loss: tensor(0.6128, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6128, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 26400  LR : 1e-05  Loss: tensor(0.6193, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6193, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 26800  LR : 1e-05  Loss: tensor(0.6294, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6294, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 27200  LR : 1e-05  Loss: tensor(0.6074, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6074, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 27600  LR : 1e-05  Loss: tensor(0.6023, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6023, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 28000  LR : 1e-05  Loss: tensor(0.5823, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.5823, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 28400  LR : 1e-05  Loss: tensor(0.6097, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6097, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 28800  LR : 1e-05  Loss: tensor(0.6608, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6608, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 29200  LR : 1e-05  Loss: tensor(0.6557, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.6557, device='cuda:0', grad_fn=<SumBackward0>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[123], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m f_x_der \u001b[38;5;241m=\u001b[39m py_kde_der(f_x,x)\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(f_x_der\u001b[38;5;241m/\u001b[39mf_x\u001b[38;5;241m.\u001b[39mview(f_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m net_5\u001b[38;5;241m.\u001b[39mloss_reg,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m opt1\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch number:\u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Iteration number:\u001b[39m\u001b[38;5;124m\"\u001b[39m,ct,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m LR :\u001b[39m\u001b[38;5;124m\"\u001b[39m,opt1\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m,torch\u001b[38;5;241m.\u001b[39msum(loss)\u001b[38;5;241m/\u001b[39mbs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReg Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m,torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(f_x_der\u001b[38;5;241m/\u001b[39mf_x\u001b[38;5;241m.\u001b[39mview(f_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39mnet_5\u001b[38;5;241m.\u001b[39mloss_reg,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39mbs))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "net_5= net_5.cuda()\n",
        "data=X\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "Y = Y.cuda()\n",
        "opt1 = torch.optim.Adam(net_5.parameters(),lr=0.00001)\n",
        "# scheduler = lr_scheduler.LinearLR(opt1, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
        "net_5.train()\n",
        "bs = 400\n",
        "for epoch in range(epochs):\n",
        "    for ct in range(0,len(X),bs):\n",
        "        x = data[ct:bs+ct].detach().cuda()\n",
        "\n",
        "        opt1.zero_grad()\n",
        "        x_hat = net_5(x).cuda()\n",
        "        # f = py_kde(x,x,0.65)\n",
        "        f_x = py_kde(x,x,0.65)\n",
        "        f_x_der = py_kde_der(f_x,x)\n",
        "    \n",
        "        loss = torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+ net_5.loss_reg,dim=1)\n",
        "        loss.backward(torch.ones_like(loss), retain_graph=True)\n",
        "        opt1.step()\n",
        "        print(\"Epoch number:\",epoch,\"  Iteration number:\",ct,\" LR :\",opt1.param_groups[0][\"lr\"], \" Loss:\",torch.sum(loss)/bs, \"Reg Loss:\",torch.sum(torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+net_5.loss_reg,dim=1)/bs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCELoss(reduction= 'none')\n",
        "def my_loss(y_pred,y_train,reg_loss):\n",
        "    loss = criterion(y_pred,y_train) +reg_loss\n",
        "    return loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "net_10 = Net(10)\n",
        "import torch.optim.lr_scheduler as lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 3.13 GiB already allocated; 44.75 MiB free; 3.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code/PowMechNB_copy3.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m data[ct:bs\u001b[39m+\u001b[39mct]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m opt1\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m x_hat \u001b[39m=\u001b[39m net_10(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# f = py_kde(x,x,0.65)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m f_x \u001b[39m=\u001b[39m py_kde(x,x,\u001b[39m0.65\u001b[39m)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/kaustubh/Code/PowMechNB_copy3.ipynb Cell 33\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m H \u001b[39m=\u001b[39m H\u001b[39m.\u001b[39mreshape(bs,d,d)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(z,H)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m J \u001b[39m=\u001b[39m batch_jacobian(H_mul, z,create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m J_int \u001b[39m=\u001b[39m\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39mdet(J)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m loss_reg \u001b[39m=\u001b[39m loss_reg \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(J_int, x,torch\u001b[39m.\u001b[39mones_like(J_int),allow_unused\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,create_graph\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n",
            "\u001b[1;32m/home/kaustubh/Code/PowMechNB_copy3.ipynb Cell 33\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_func_sum\u001b[39m(z):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(z)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mjacobian(_func_sum, z, create_graph\u001b[39m=\u001b[39;49mcreate_graph))\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/functional.py:686\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    684\u001b[0m jac_i: Tuple[List[torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inputs)))  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(out\u001b[39m.\u001b[39mnelement()):\n\u001b[0;32m--> 686\u001b[0m     vj \u001b[39m=\u001b[39m _autograd_grad((out\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)[j],), inputs,\n\u001b[1;32m    687\u001b[0m                         retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, create_graph\u001b[39m=\u001b[39;49mcreate_graph)\n\u001b[1;32m    689\u001b[0m     \u001b[39mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    690\u001b[0m         \u001b[39mif\u001b[39;00m vj_el \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/functional.py:167\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m,) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(inputs)\n\u001b[1;32m    166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(new_outputs, inputs, new_grad_outputs, allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m                                create_graph\u001b[39m=\u001b[39;49mcreate_graph, retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    169\u001b[0m                                is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 3.13 GiB already allocated; 44.75 MiB free; 3.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "net_10= net_10.cuda()\n",
        "data=X.cuda()\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "Y = Y.cuda()\n",
        "opt1 = torch.optim.Adam(net_10.parameters(),lr=0.001)\n",
        "# scheduler = lr_scheduler.LinearLR(opt1, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
        "net_10.train()\n",
        "bs = 500\n",
        "for epoch in range(epochs):\n",
        "    for ct in range(0,len(X),bs):\n",
        "        x = data[ct:bs+ct].detach().cuda()\n",
        "\n",
        "        opt1.zero_grad()\n",
        "        x_hat = net_10(x)\n",
        "        # f = py_kde(x,x,0.65)\n",
        "        f_x = py_kde(x,x,0.65)\n",
        "        f_x_der = py_kde_der(f_x,x)\n",
        "        loss = criterion(torch.squeeze(x_hat[0:,0:,0]),torch.squeeze(Y[ct:bs+ct])) +torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+ net_10.loss_reg,dim=1)\n",
        "        loss.backward(torch.ones_like(loss), retain_graph=True)\n",
        "        opt1.step()\n",
        "        print(\"Epoch number:\",epoch,\"  Iteration number:\",ct,\" LR :\",opt1.param_groups[0][\"lr\"], \" Loss:\",torch.sum(loss)/bs, \"Reg Loss:\",torch.sum(torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+net_10.loss_reg,dim=1)/bs))\n",
        "    # if(epoch%20==19):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upper bound on Epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=X\n",
        "\n",
        "\n",
        "\n",
        "losses= torch.zeros(X.shape[0])\n",
        "bs = 1000\n",
        "n = bs\n",
        "h = 0.65\n",
        "d = X.shape[1]\n",
        "alpha= 0.01\n",
        "for ct in range(0,len(X),bs):\n",
        "    x = data[ct:bs+ct].detach()\n",
        "    x_hat = net_5(x)\n",
        "    f = py_kde(x,x,0.65)\n",
        "    f_der = py_kde_der(f,x)\n",
        "    ci = CI_KDE(f,n,h,d,alpha)\n",
        "    loss =torch.max(torch.linalg.norm(f_der/(f-ci).view(f.shape[0],1)+net_5.loss_reg,dim=1),torch.linalg.norm(f_der/(f+ci).view(f.shape[0],1)+net_5.loss_reg,dim=1)) \n",
        "    losses[ct:bs+ct] =loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# net = Net(1)\n",
        "net = torch.load(\"Models/net_1_cov\")\n",
        "# net.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=1000,\n",
        "                                        shuffle=True, num_workers=2,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(net.parameters(),lr=0.00003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 44\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m net\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m net\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m test_model(net,trainloader_priv)\n",
            "File \u001b[0;32m~/Code3/cov_help.py:330\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m    329\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m--> 330\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    331\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m    332\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 44\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     J \u001b[39m=\u001b[39m batch_jacobian(H_mul, z, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     J_int \u001b[39m=\u001b[39m\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39mdet(J)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     loss_reg \u001b[39m=\u001b[39m loss_reg \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(J_int, x,torch\u001b[39m.\u001b[39;49mones_like(J_int),allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,create_graph\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_reg \u001b[39m=\u001b[39m loss_reg\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m z\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "net.to(torch.device('cuda'))\n",
        "net.eval()\n",
        "test_model(net,trainloader_priv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 45\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(torch\u001b[39m.\u001b[39msqueeze(outputs),torch\u001b[39m.\u001b[39msqueeze(labels))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     losses\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39msum()\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 45\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     J \u001b[39m=\u001b[39m batch_jacobian(H_mul, z, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     J_int \u001b[39m=\u001b[39m\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39mdet(J)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     loss_reg \u001b[39m=\u001b[39m loss_reg \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(J_int, x,torch\u001b[39m.\u001b[39;49mones_like(J_int),allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,create_graph\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_reg \u001b[39m=\u001b[39m loss_reg\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m z\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "net.eval()\n",
        "test_loader = trainloader_priv\n",
        "correct = 0\n",
        "total = 0\n",
        "losses =0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(torch.squeeze(outputs),torch.squeeze(labels))\n",
        "        losses+=loss.sum()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0021)"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "losses/len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (H_net1): Sequential(\n",
              "    (0): Linear(in_features=54, out_features=128, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (3): Sigmoid()\n",
              "    (4): Linear(in_features=256, out_features=2916, bias=True)\n",
              "  )\n",
              "  (X_net): Sequential(\n",
              "    (0): Linear(in_features=54, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=128, out_features=7, bias=True)\n",
              "    (5): Softmax(dim=2)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[6.6744e-02, 9.2670e-01, 2.1241e-03, 1.3546e-03, 9.6444e-04,\n",
              "          1.2291e-03, 8.8563e-04]],\n",
              "\n",
              "        [[4.8453e-02, 9.4747e-01, 1.3700e-03, 8.5512e-04, 5.7667e-04,\n",
              "          7.4947e-04, 5.2757e-04]],\n",
              "\n",
              "        [[5.4603e-02, 9.3788e-01, 2.3995e-03, 1.5850e-03, 1.1224e-03,\n",
              "          1.3911e-03, 1.0192e-03]]], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net(X[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:wtr3pi52) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59e96e20c38f40ce8a10d74c216eba84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.008 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.529151…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▄▄▆▃▄▄▇▆▄▄█▃▆▇▂▁▄▅▃▂▃▃▁▅▄▃▂▃▄▆▂▂▂▂▅▂▁▃</td></tr><tr><td>loss_reg</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▂▂▁▂▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.00157</td></tr><tr><td>loss_reg</td><td>37.95282</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ghostly-spirit-5</strong> at: <a href='https://wandb.ai/ponkshekaustubh11/covertype%202/runs/wtr3pi52' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covertype%202/runs/wtr3pi52</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231031_151633-wtr3pi52/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:wtr3pi52). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.12 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/kaustubh/Code3/wandb/run-20231031_151737-r3tvdm7v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ponkshekaustubh11/covertype%202/runs/r3tvdm7v' target=\"_blank\">chilling-treat-6</a></strong> to <a href='https://wandb.ai/ponkshekaustubh11/covertype%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ponkshekaustubh11/covertype%202' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covertype%202</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ponkshekaustubh11/covertype%202/runs/r3tvdm7v' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covertype%202/runs/r3tvdm7v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 46\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m run \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcovertype 2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m lr_schedule \u001b[39m=\u001b[39m LearnerRateScheduler(\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m, base_learning_rate\u001b[39m=\u001b[39m\u001b[39m0.00003\u001b[39m, decay_rate \u001b[39m=\u001b[39m \u001b[39m0.99\u001b[39m, decay_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m train_model_priv(net,trainloader_priv,optim,\u001b[39m4\u001b[39;49m,h\u001b[39m=\u001b[39;49m\u001b[39m0.82\u001b[39;49m,rate\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m),only_reg_flag\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,lr_schedular\u001b[39m=\u001b[39;49mlr_schedule)\n",
            "File \u001b[0;32m~/Code3/cov_help.py:161\u001b[0m, in \u001b[0;36mtrain_model_priv\u001b[0;34m(net, trainloader, optimizer, epochs, h, rate, device, print_cond, only_reg_flag, lr_schedular)\u001b[0m\n\u001b[1;32m    159\u001b[0m labels \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    160\u001b[0m f \u001b[39m=\u001b[39m py_kde(inputs,inputs,h)\n\u001b[0;32m--> 161\u001b[0m f_der \u001b[39m=\u001b[39m py_kde_der(f,inputs)\n\u001b[1;32m    163\u001b[0m \u001b[39m# zero the parameter gradients\u001b[39;00m\n\u001b[1;32m    164\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/Code3/cov_help.py:90\u001b[0m, in \u001b[0;36mpy_kde_der\u001b[0;34m(p_x, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpy_kde_der\u001b[39m(p_x,x):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# x.requires_grad = True\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39m# p_x = py_kde(x,X_t,h)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(p_x,x,torch\u001b[39m.\u001b[39;49mones_like(p_x),allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run = wandb.init(project=\"covertype 2\")\n",
        "\n",
        "lr_schedule = LearnerRateScheduler('step', base_learning_rate=0.00003, decay_rate = 0.99, decay_steps=1)\n",
        "train_model_priv(net,trainloader_priv,optim,4,h=0.82,rate=10,device=torch.device('cuda'),only_reg_flag=2,lr_schedular=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "net =  torch.load(\"Models/net_1_cov\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_emb,losses = create_model_embs(net,trainloader_priv,device= torch.device('cuda'),l=len(X),h=0.82)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.3786)"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(losses).sum()/len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        " \n",
        " \n",
        "# Creating dataset\n",
        "a = losses.detach()\n",
        " \n",
        "# Creating histogram\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(a, bins = np.arange(0.1,5,0.03))\n",
        " \n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_emb_train, X_emb_test, Y_train, Y_test = train_test_split(X_emb, Y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_emb_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_emb_train,Y_train), batch_size=1000,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "test_emb_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_emb_test,Y_test), batch_size=1000,\n",
        "                                        shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10], loss: 1.677\n",
            "Epoch [20], loss: 1.676\n",
            "Epoch [30], loss: 1.676\n",
            "Epoch [40], loss: 1.676\n",
            "Epoch [50], loss: 1.676\n",
            "Epoch [60], loss: 1.676\n",
            "Epoch [70], loss: 1.676\n",
            "Epoch [80], loss: 1.676\n",
            "Epoch [90], loss: 1.677\n",
            "Epoch [100], loss: 1.676\n"
          ]
        }
      ],
      "source": [
        "net.cpu()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=1)\n",
        "\n",
        "        )\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=1e-4)\n",
        "train_emb(model, train_emb_loader, criterion, optimizer, num_epochs=100,device=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "o = autoencoder1(X_train.detach())\n",
        "X_emb_train = torch.squeeze(autoencoder1.y).detach()\n",
        "o = autoencoder1(X_test.detach())\n",
        "X_emb_test = torch.squeeze(autoencoder1.y).detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 49 %\n"
          ]
        }
      ],
      "source": [
        "test_model(model,test_emb_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.8339999914169312\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# o = autoencoder1(X)\n",
        "# X_embs = torch.squeeze(autoencoder1.y.detach())\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(12, 48),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(48, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid(\n",
        "\n",
        "    )\n",
        ")\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "n_epochs = 2000\n",
        "batch_size = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X_emb_train), batch_size):\n",
        "        Xbatch = X_emb_train[i:i+batch_size]\n",
        "        y_pred = model(Xbatch)\n",
        "        ybatch = Y_train[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #     print(loss)\n",
        "    # print(f'Finished epoch {epoch}, latest loss {loss}')\n",
        "# compute accuracy (no_grad is optional)\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_emb_test)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.871999979019165\n"
          ]
        }
      ],
      "source": [
        "model1 = nn.Sequential(\n",
        "    nn.Linear(12, 48),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(48, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid(\n",
        "\n",
        "    )\n",
        ")\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "n_epochs = 1000\n",
        "batch_size = 1000\n",
        " \n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        Xbatch = X_train[i:i+batch_size]\n",
        "        y_pred = model1(Xbatch)\n",
        "        ybatch = Y_train[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        optimizer1.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = model1(X_test)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = \"Models/cov_new_norm_1_2\"\n",
        "state_dict = torch.load(model_path)\n",
        "from cov_help import *\n",
        "    # Create an instance of Net\n",
        "net = Net(1)\n",
        "net.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H_net1.0.weight\n",
            "H_net1.0.bias\n",
            "H_net1.2.weight\n",
            "H_net1.2.bias\n",
            "H_net1.4.weight\n",
            "H_net1.4.bias\n",
            "X_net.0.weight\n",
            "X_net.0.bias\n",
            "X_net.2.weight\n",
            "X_net.2.bias\n",
            "X_net.4.weight\n",
            "X_net.4.bias\n",
            "X_net.6.weight\n",
            "X_net.6.bias\n"
          ]
        }
      ],
      "source": [
        "# prune_param = []\n",
        "# for param in net.parameters():\n",
        "#     print(param)\n",
        "    # name = param.name\n",
        "    # if( \"H_net1\" in name):\n",
        "    #     prune_param.append((param,\"weight\"))\n",
        "    #     print(param)\n",
        "# prune_param = []\n",
        "for name, param in net.named_parameters():\n",
        "    print(name)\n",
        "    # name = param.name\n",
        "    # if( \"H_net1\" in name):\n",
        "    #     prune_param.append((param,\"weight\"))\n",
        "    #     print(param)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0326,  0.0385,  0.0601,  ..., -0.1135, -0.1319, -0.0933],\n",
              "        [ 0.0218, -0.0314, -0.0063,  ...,  0.0763, -0.1219, -0.1075],\n",
              "        [ 0.1208, -0.0838,  0.1304,  ..., -0.0679,  0.1198, -0.1198],\n",
              "        ...,\n",
              "        [ 0.0521, -0.0298, -0.0997,  ...,  0.0153,  0.0183, -0.1232],\n",
              "        [ 0.1172,  0.0730, -0.0087,  ...,  0.1323, -0.1369,  0.0438],\n",
              "        [-0.1024, -0.1241,  0.0534,  ...,  0.0743,  0.0742,  0.0840]],\n",
              "       device='cuda:0', requires_grad=True)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.to(torch.device(\"cuda\"))\n",
        "net.H_net1[0].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"Models/cov_new_norm_1_2\"\n",
        "state_dict = torch.load(model_path)\n",
        "from cov_help import *\n",
        "    # Create an instance of Net\n",
        "net = Net(1).to(torch.device(\"cuda\"))\n",
        "net.load_state_dict(state_dict)\n",
        "params_to_prune = [(net.H_net1[0],\"weight\"),(net.H_net1[2],\"weight\"),(net.H_net1[4],\"weight\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "\n",
        "prune.global_unstructured(\n",
        "    params_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.6,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "X,Y = cov_data_loader(data_path,norm=norm)\n",
        "max_dist = torch.cdist(X, X).max()\n",
        "\n",
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=1000,\n",
        "                                        shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_emb,losses = create_model_embs2(net,trainloader_priv,device= torch.device('cuda'),l=len(X),h=0.82)\n",
        "losses,indices = torch.sort(losses*max_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.4166, 0.4169, 0.4170,  ..., 1.8810, 1.9243, 1.9842])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "losses"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
