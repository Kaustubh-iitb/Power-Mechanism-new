/home/kaustubh/llm/lib/python3.10/site-packages/opacus/privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.
  warnings.warn(
/home/kaustubh/llm/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.
  warnings.warn(
/home/kaustubh/llm/lib/python3.10/site-packages/opacus/accountants/analysis/prv/prvs.py:50: RuntimeWarning: invalid value encountered in log
  z = np.log((np.exp(t) + q - 1) / q)
02/13/2024 13:28:02:WARNING:Ignoring drop_last as it is not compatible with DPDataLoader.
/home/kaustubh/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
Traceback (most recent call last):
  File "/home/kaustubh/Code3/Forrest Cover Type/non_priv_cov.py", line 108, in <module>
    main()
  File "/home/kaustubh/Code3/Forrest Cover Type/non_priv_cov.py", line 101, in main
    train_emb(model2, data_loader, criterion, optimizer2, num_epochs=num_epochs,device=torch.device('cuda'),test_loader = test_loader)
  File "/home/kaustubh/Code3/Forrest Cover Type/cov_help.py", line 594, in train_emb
    labels = labels.to(device)
KeyboardInterrupt