
tensor(2.7668)
Traceback (most recent call last):
  File "/home/kaustubh/Code3/cov_trainer.py", line 77, in <module>
    main(data_path=data_path,batch_size=batch_size,num_epochs=num_epochs,learning_rate=learning_rate,train_flag=train_flag)
  File "/home/kaustubh/Code3/cov_trainer.py", line 63, in main
    train_model_priv(net,trainloader_priv,optim,num_epochs,h=0.82,rate=10,device=torch.device('cuda'),only_reg_flag=0,lr_schedular=lr_schedule)
  File "/home/kaustubh/Code3/cov_help.py", line 166, in train_model_priv
    outputs = net(inputs)
  File "/home/kaustubh/llm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kaustubh/Code3/cov_help.py", line 67, in forward
    J = batch_jacobian(H_mul, z, create_graph=True)
  File "/home/kaustubh/Code3/cov_help.py", line 53, in batch_jacobian
    return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)
  File "/home/kaustubh/llm/lib/python3.10/site-packages/torch/autograd/functional.py", line 686, in jacobian
    vj = _autograd_grad((out.reshape(-1)[j],), inputs,
  File "/home/kaustubh/llm/lib/python3.10/site-packages/torch/autograd/functional.py", line 167, in _autograd_grad
    return torch.autograd.grad(new_outputs, inputs, new_grad_outputs, allow_unused=True,
  File "/home/kaustubh/llm/lib/python3.10/site-packages/torch/autograd/__init__.py", line 303, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 15.73 GiB total capacity; 9.40 GiB already allocated; 55.69 MiB free; 9.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF