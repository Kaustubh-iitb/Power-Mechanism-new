{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import dcMinMaxFunctions as dc\n",
        "# import dcor\n",
        "from scipy.misc import derivative\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import torch\n",
        "from scipy import stats\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"data/Churn_Modelling.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowNumber</th>\n",
              "      <th>CustomerId</th>\n",
              "      <th>Surname</th>\n",
              "      <th>CreditScore</th>\n",
              "      <th>Geography</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Tenure</th>\n",
              "      <th>Balance</th>\n",
              "      <th>NumOfProducts</th>\n",
              "      <th>HasCrCard</th>\n",
              "      <th>IsActiveMember</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Exited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>15634602</td>\n",
              "      <td>Hargrave</td>\n",
              "      <td>619</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>2</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101348.88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>15647311</td>\n",
              "      <td>Hill</td>\n",
              "      <td>608</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>83807.86</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>112542.58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>15619304</td>\n",
              "      <td>Onio</td>\n",
              "      <td>502</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>42</td>\n",
              "      <td>8</td>\n",
              "      <td>159660.80</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113931.57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>15701354</td>\n",
              "      <td>Boni</td>\n",
              "      <td>699</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93826.63</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>15737888</td>\n",
              "      <td>Mitchell</td>\n",
              "      <td>850</td>\n",
              "      <td>Spain</td>\n",
              "      <td>Female</td>\n",
              "      <td>43</td>\n",
              "      <td>2</td>\n",
              "      <td>125510.82</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>79084.10</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9996</td>\n",
              "      <td>15606229</td>\n",
              "      <td>Obijiaku</td>\n",
              "      <td>771</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>39</td>\n",
              "      <td>5</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>96270.64</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9997</td>\n",
              "      <td>15569892</td>\n",
              "      <td>Johnstone</td>\n",
              "      <td>516</td>\n",
              "      <td>France</td>\n",
              "      <td>Male</td>\n",
              "      <td>35</td>\n",
              "      <td>10</td>\n",
              "      <td>57369.61</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101699.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9998</td>\n",
              "      <td>15584532</td>\n",
              "      <td>Liu</td>\n",
              "      <td>709</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>36</td>\n",
              "      <td>7</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>42085.58</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9999</td>\n",
              "      <td>15682355</td>\n",
              "      <td>Sabbatini</td>\n",
              "      <td>772</td>\n",
              "      <td>Germany</td>\n",
              "      <td>Male</td>\n",
              "      <td>42</td>\n",
              "      <td>3</td>\n",
              "      <td>75075.31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92888.52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>10000</td>\n",
              "      <td>15628319</td>\n",
              "      <td>Walker</td>\n",
              "      <td>792</td>\n",
              "      <td>France</td>\n",
              "      <td>Female</td>\n",
              "      <td>28</td>\n",
              "      <td>4</td>\n",
              "      <td>130142.79</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>38190.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
              "0             1    15634602   Hargrave          619    France  Female   42   \n",
              "1             2    15647311       Hill          608     Spain  Female   41   \n",
              "2             3    15619304       Onio          502    France  Female   42   \n",
              "3             4    15701354       Boni          699    France  Female   39   \n",
              "4             5    15737888   Mitchell          850     Spain  Female   43   \n",
              "...         ...         ...        ...          ...       ...     ...  ...   \n",
              "9995       9996    15606229   Obijiaku          771    France    Male   39   \n",
              "9996       9997    15569892  Johnstone          516    France    Male   35   \n",
              "9997       9998    15584532        Liu          709    France  Female   36   \n",
              "9998       9999    15682355  Sabbatini          772   Germany    Male   42   \n",
              "9999      10000    15628319     Walker          792    France  Female   28   \n",
              "\n",
              "      Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
              "0          2       0.00              1          1               1   \n",
              "1          1   83807.86              1          0               1   \n",
              "2          8  159660.80              3          1               0   \n",
              "3          1       0.00              2          0               0   \n",
              "4          2  125510.82              1          1               1   \n",
              "...      ...        ...            ...        ...             ...   \n",
              "9995       5       0.00              2          1               0   \n",
              "9996      10   57369.61              1          1               1   \n",
              "9997       7       0.00              1          0               1   \n",
              "9998       3   75075.31              2          1               0   \n",
              "9999       4  130142.79              1          1               0   \n",
              "\n",
              "      EstimatedSalary  Exited  \n",
              "0           101348.88       1  \n",
              "1           112542.58       0  \n",
              "2           113931.57       1  \n",
              "3            93826.63       0  \n",
              "4            79084.10       0  \n",
              "...               ...     ...  \n",
              "9995         96270.64       0  \n",
              "9996        101699.77       0  \n",
              "9997         42085.58       1  \n",
              "9998         92888.52       1  \n",
              "9999         38190.78       0  \n",
              "\n",
              "[10000 rows x 14 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6493816315762113"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n=1000\n",
        "d=12\n",
        "n**(-1./(d+4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tHXngvYhO3MX"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"data/Churn_Modelling.csv\")\n",
        "# df=df.drop(['duration', 'pdays'],axis=1) # duration gives away the answer, and pdays has too much missing info\n",
        "\n",
        "X = df.loc[:, df.columns != 'Exited'].replace(dict(yes=True, no=False))\n",
        "Y = df.loc[:, ['Exited']].replace(dict(yes=True, no=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_columns = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n",
        "numerical_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
        "outputs = ['Exited']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "for category in categorical_columns:\n",
        "    df[category] = df[category].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def OHE(x):\n",
        "    dim = np.max(x)\n",
        "    y = np.zeros((len(x),dim+1))\n",
        "    for i in range(len(x)):\n",
        "        y[i][x[i]] = 1\n",
        "    return(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "geo = OHE(df['Geography'].cat.codes.values)\n",
        "gen =  np.asarray(df['Gender'].cat.codes.values)\n",
        "hcc =  np.asarray(df['HasCrCard'].cat.codes.values)\n",
        "iam =  np.asarray(df['IsActiveMember'].cat.codes.values)\n",
        "\n",
        "categorical_data = np.stack(( gen, hcc, iam), axis=1)\n",
        "# categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
        "numerical_data = np.stack([df[col].values for col in numerical_columns], 1)\n",
        "# numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
        "X = np.concatenate((numerical_data, categorical_data,geo), axis=1)\n",
        "Y = df[outputs].values\n",
        "# outputs = torch.tensor(df[outputs].values).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    x_normed = x / x.max(0, keepdim=True)[0]\n",
        "    return x_normed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = torch.Tensor(X)\n",
        "Y = torch.Tensor(Y)\n",
        "X = normalize(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RSikHVAlO3MY"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating Data Densities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gaussian Kernel Density Estimation & Derivative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gau_ker(u):\n",
        "    return torch.pow(2*torch.tensor(torch.pi),u.shape[1]/(-2))*torch.exp(torch.bmm(u.view(u.shape[0], 1, u.shape[1]), u.view(u.shape[0],  u.shape[1],1))/(-2))\n",
        "\n",
        "\n",
        "def py_kde(x,X_t,h):\n",
        "    norm = X_t.shape[0]*(h**x.shape[1])\n",
        "    prob = torch.zeros(x.shape[0]).cuda()\n",
        "    for i in range(len(X_t)):\n",
        "        prob+= (torch.squeeze(gau_ker((x - X_t[i])/h))/norm).cuda()\n",
        "    return(prob)\n",
        "\n",
        "\n",
        "def py_kde_der(p_x,x):\n",
        "    # x.requires_grad = True\n",
        "    # p_x = py_kde(x,X_t,h)\n",
        "    return (torch.autograd.grad(p_x,x,torch.ones_like(p_x),allow_unused=True,create_graph=True)[0]).cuda()\n",
        "\n",
        "\n",
        "def gau_ker_der(X,h):\n",
        "    N= X.shape[0]\n",
        "    d = X.shape[1]\n",
        "    grad = torch.zeros(X.shape)\n",
        "    for n in range(N):\n",
        "        for i in range(d):\n",
        "            for j in range(N):\n",
        "                grad[n][i]+= torch.exp(-1*torch.dot((X[n]-X[j]),(X[n]-X[j]))/(2*h*h))*(X[n][i] -X[j][i]) /(N*(h**(d+2))*((2*math.pi)**(d/2)))\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Confidence Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def CI_KDE(p_x,n,h,d,alpha):\n",
        "    return( stats.norm.ppf(1-alpha/2)*torch.sqrt(p_x/((2**d)*math.sqrt(torch.pi**d)*n*h**(d))) )\n",
        "\n",
        "def CI_KDE_der(p_x_der,p_x,n,h,d,alpha):\n",
        "    return( p_x_der*stats.norm.ppf(1-alpha/2)*torch.sqrt(1/(p_x.unsqueeze(dim=1)*(2**d)*math.sqrt(torch.pi**d)*n*h**(d))) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "h=0.65\n",
        "x = X[0:1000].detach().cuda()\n",
        "x.requires_grad = True\n",
        "f = py_kde(x,x,h)\n",
        "f_der = py_kde_der(f,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = X.shape[0]\n",
        "d = X.shape[1]\n",
        "alpha =0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "ci = CI_KDE(f,n,h,d,alpha)\n",
        "ci_der = CI_KDE_der(f_der,f,n,h,d,alpha)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0403, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.norm((ci_der/f.view(-1,1))[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.8513, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.norm(ci_der/f.view(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 6.8255e-07, -5.1870e-07,  5.3233e-06,  ..., -1.8655e-06,\n",
              "          8.5586e-07,  1.0096e-06],\n",
              "        [ 6.5446e-07, -1.2829e-07,  4.2369e-06,  ...,  1.6794e-06,\n",
              "          1.0232e-06, -2.7026e-06],\n",
              "        [ 3.0163e-06, -7.6364e-07, -4.3834e-06,  ..., -1.9228e-06,\n",
              "          1.1064e-06,  8.1638e-07],\n",
              "        ...,\n",
              "        [ 1.0246e-06, -3.1481e-06, -2.0484e-06,  ...,  1.6132e-06,\n",
              "          8.9832e-07, -2.5115e-06],\n",
              "        [-2.8924e-06,  1.7713e-06,  1.2602e-06,  ...,  1.8684e-06,\n",
              "         -2.9437e-06,  1.0753e-06],\n",
              "        [-2.0102e-06,  1.4673e-07,  5.4403e-06,  ..., -1.5799e-06,\n",
              "          8.3815e-07,  7.4178e-07]], device='cuda:0', grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ci_der"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.0024, -0.0018,  0.0184,  0.0164,  0.0092, -0.0019,  0.0221, -0.0084,\n",
              "        -0.0174, -0.0064,  0.0030,  0.0035], device='cuda:0',\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ci_der[0]/float(f[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.H_net1 = nn.Sequential(\n",
        "            nn.Linear(12, 48),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(48, 24),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(24, 144).cuda()\n",
        "\n",
        "        )\n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(12, 16),\n",
        "            nn.ReLU(),\n",
        "            # nn.Linear(48, 48),\n",
        "            # nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            H12 = self.H_net1(z)\n",
        "            H12= H12.reshape(z.shape[0],d,d)\n",
        "            x12 = torch.matmul(z,H12)\n",
        "            return(x12)\n",
        "    \n",
        "        \n",
        "        def batch_jacobian(func, z, create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x.cuda()\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            H = self.H_net1(z).cuda()\n",
        "            H = H.reshape(bs,d,d)\n",
        "            z = torch.matmul(z,H).cuda()\n",
        "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).cuda()\n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.7670]],\n",
              "\n",
              "        [[0.5325]],\n",
              "\n",
              "        [[0.9684]]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net(10).cuda()\n",
        "net(X_train[0:3].cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  0.2150,   0.0528,  -1.4557,  -0.8471,  -0.1672,   0.2958,  -1.3538,\n",
              "           0.3180,   0.4657,   0.2482,  -0.3461,  -0.2947],\n",
              "        [ -1.4400,  -0.9708,  -2.4521,   5.6186,  -0.1683,   0.3752,  -3.5791,\n",
              "          -0.9337,  -4.5138,  -3.1731,  -0.5164,  -3.8039],\n",
              "        [-33.7263,  24.5918,  16.5738, -42.7060, -22.5589,  12.1098,   4.1549,\n",
              "         -13.3294,  39.4987,  16.8790,  18.9683,  15.5196]], device='cuda:0',\n",
              "       grad_fn=<SubBackward0>)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.loss_reg - f_der[0:3]/f[0:3].view(f[0:3].shape[0],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cCPV_zl0O3Mc"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCELoss(reduction= 'none')\n",
        "def my_loss(y_pred,y_train,reg_loss):\n",
        "    loss = criterion(y_pred,y_train) +reg_loss\n",
        "    return loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "net_5 = Net(10)\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_priv(net,trainloader,optimizer,epochs,h,rate=10,device= torch.device('cpu'),print_cond = True,only_reg_flag=0):\n",
        "    # scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        # scheduler.step()\n",
        "        running_loss = 0.0\n",
        "        net = net.to(device)\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            \n",
        "            inputs = data[0].to(device)\n",
        "            inputs.requires_grad = True\n",
        "            labels = data[1].to(device)\n",
        "            f = py_kde(inputs,inputs,h)\n",
        "            f_der = py_kde_der(f,inputs)\n",
        "          \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            if(only_reg_flag):\n",
        "                loss = torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1)\n",
        "            else:\n",
        "                loss = criterion(torch.squeeze(outputs),torch.squeeze(labels)) + torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1)\n",
        "            loss.backward(torch.ones_like(loss),retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "      \n",
        "            # print statistics\n",
        "            # print(loss.sum().shape)\n",
        "            running_loss += loss.sum()\n",
        "            # if i % 100 == 99:    # print every 2000 mini-batches\n",
        "        if(epoch%rate==rate-1):\n",
        "            if(print_cond):\n",
        "                print('%d loss: %.10f' %\n",
        "                        (epoch + 1, running_loss /(rate*len(trainloader.dataset))))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "#write torch.adam optimizer which first warms up and then decays\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loss: 3.8269534111\n",
            "2 loss: 3.0497708321\n",
            "3 loss: 2.9040632248\n",
            "4 loss: 2.7770833969\n",
            "5 loss: 2.6772150993\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train = torch.utils.data.TensorDataset(X_train,Y_train)\n",
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=1000,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "net_5 = Net(3)\n",
        "optim = torch.optim.Adam(net_5.parameters(),lr=0.01)\n",
        "train_model_priv(net_5,trainloader_priv,optim,5,h=0.65,rate=1,device=torch.device('cuda'),only_reg_flag=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loss: 2.8099992275\n",
            "2 loss: 2.4453482628\n",
            "3 loss: 2.0053462982\n",
            "4 loss: 1.8305778503\n",
            "5 loss: 1.4373526573\n",
            "6 loss: 1.2463039160\n",
            "7 loss: 1.0928441286\n",
            "8 loss: 1.0100624561\n",
            "9 loss: 0.9492591619\n",
            "10 loss: 0.9543683529\n",
            "11 loss: 0.8591307402\n",
            "12 loss: 0.8429867029\n",
            "13 loss: 0.8223188519\n",
            "14 loss: 0.8133831620\n",
            "15 loss: 0.7906615734\n",
            "16 loss: 0.7793525457\n",
            "17 loss: 0.7564795017\n",
            "18 loss: 0.7168283463\n",
            "19 loss: 0.7477931976\n",
            "20 loss: 0.7275117636\n",
            "21 loss: 0.7567098737\n",
            "22 loss: 0.7445353866\n",
            "23 loss: 0.7447450757\n",
            "24 loss: 0.7080577016\n",
            "25 loss: 0.6747940183\n",
            "26 loss: 0.6519592404\n",
            "27 loss: 0.6679731011\n",
            "28 loss: 0.7095085979\n",
            "29 loss: 0.7323722243\n",
            "30 loss: 0.6936945319\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "optim = torch.optim.Adam(net_5.parameters(),lr=0.01)\n",
        "train_model_priv(net_5,trainloader_priv,optim,30,h=0.65,rate=1,device=torch.device('cuda'),only_reg_flag=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loss: 1.2841753960\n",
            "2 loss: 1.1753804684\n",
            "3 loss: 1.1426836252\n",
            "4 loss: 1.1374456882\n",
            "5 loss: 1.1311126947\n",
            "6 loss: 1.0961644650\n",
            "7 loss: 1.1231994629\n",
            "8 loss: 1.1206990480\n",
            "9 loss: 1.1151359081\n",
            "10 loss: 1.0391232967\n",
            "11 loss: 1.0263444185\n",
            "12 loss: 1.0227079391\n",
            "13 loss: 1.0137541294\n",
            "14 loss: 0.9767276049\n",
            "15 loss: 1.0190696716\n",
            "16 loss: 1.0212602615\n",
            "17 loss: 0.9718999863\n",
            "18 loss: 1.0058130026\n",
            "19 loss: 0.9561971426\n",
            "20 loss: 0.9638320804\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "train_model_priv(net_5,trainloader_priv,optim,20,h=0.65,rate=1,device=torch.device('cuda'),only_reg_flag=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loss: 1.0279887915\n",
            "2 loss: 0.8758187294\n",
            "3 loss: 0.8489959836\n",
            "4 loss: 0.8448605537\n",
            "5 loss: 0.8281965852\n",
            "6 loss: 0.8061122298\n",
            "7 loss: 0.8135287762\n",
            "8 loss: 0.8008967638\n",
            "9 loss: 0.8267527223\n",
            "10 loss: 0.8310554624\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "optim = torch.optim.Adam(net_5.parameters(),lr=0.003)\n",
        "train_model_priv(net_5,trainloader_priv,optim,10,h=0.65,rate=1,device=torch.device('cuda'),only_reg_flag=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#write function to save net_5 model\n",
        "# torch.save(net_5.state_dict(), \"net_5.pth\")\n",
        "# not just weights but entire modek\n",
        "torch.save(net_5, \"Models/net_6_complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upper bound on Epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=X\n",
        "\n",
        "# m1 = torch.load(\"Models/net_5_5.pth\")\n",
        "m1 = net_5\n",
        "losses= torch.zeros(X.shape[0])\n",
        "bs = 1000\n",
        "n = bs\n",
        "h = 0.65\n",
        "d = X.shape[1]\n",
        "alpha= 0.0001\n",
        "for ct in range(0,len(X),bs):\n",
        "    x = data[ct:bs+ct].detach()\n",
        "    x_hat = m1(x)\n",
        "    f = py_kde(x,x,0.65)\n",
        "    f_der = py_kde_der(f,x)\n",
        "    ci = CI_KDE(f,n,h,d,alpha)\n",
        "    ci_der = CI_KDE_der(f_der,f,n,h,d,alpha)\n",
        "    loss =torch.max(torch.linalg.norm(f_der/(f-ci).view(f.shape[0],1)+m1.loss_reg,dim=1),torch.linalg.norm(f_der/(f+ci).view(f.shape[0],1)+m1.loss_reg,dim=1)) + torch.norm(ci_der/f.view(-1,1),dim=1)\n",
        "    losses[ct:bs+ct] =loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.2897, 0.3066, 0.3142,  ..., 1.8749, 1.9794, 1.9876],\n",
              "       grad_fn=<SortBackward0>)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "#arrange X and Y in ascending order of losses\n",
        "losses,indices = torch.sort(losses)\n",
        "X = X[indices]\n",
        "Y = Y[indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        " \n",
        " \n",
        "# Creating dataset\n",
        "a = losses.detach()[0:10000]*2.7\n",
        " \n",
        "# Creating histogram\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(a, bins = np.arange(0,10,0.1))\n",
        " \n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        " \n",
        " \n",
        "# Creating dataset\n",
        "a = losses.detach()[0:10000]*2.7\n",
        " \n",
        "# Creating histogram\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(a, bins = np.arange(0,10,0.1))\n",
        " \n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_eps = 2\n",
        "ind =(losses*2.7 < set_eps).sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5369\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X[0:ind], Y[0:ind], test_size=0.2)\n",
        "print(len(X_train))\n",
        "o = m1(X_train.detach())\n",
        "X_emb_train = torch.squeeze(m1.y).detach()\n",
        "o = m1(X_test.detach())\n",
        "X_emb_test = torch.squeeze(m1.y).detach()\n",
        "o = m1(X[ind:-1].detach())\n",
        "X_emb_test2 = torch.squeeze(m1.y).detach()\n",
        "Y_test2 = Y[ind:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy 0.8696219325065613 5369\n",
            "Test Accuracy 0.8585256934165955\n",
            "Train Accuracy 0.8752095699310303 5369\n",
            "Test Accuracy 0.8585256934165955\n",
            "Train Accuracy 0.8763270974159241 5369\n",
            "Test Accuracy 0.8570365309715271\n",
            "Train Accuracy 0.8781896233558655 5369\n",
            "Test Accuracy 0.8600149154663086\n",
            "Train Accuracy 0.8787484169006348 5369\n",
            "Test Accuracy 0.8622487187385559\n",
            "Train Accuracy 0.8783758878707886 5369\n",
            "Test Accuracy 0.8585256934165955\n",
            "Train Accuracy 0.8794934153556824 5369\n",
            "Test Accuracy 0.8607594966888428\n",
            "Train Accuracy 0.8791208863258362 5369\n",
            "Test Accuracy 0.8607594966888428\n",
            "Train Accuracy 0.8794934153556824 5369\n",
            "Test Accuracy 0.8607594966888428\n",
            "Train Accuracy 0.8796796798706055 5369\n",
            "Test Accuracy 0.8592703342437744\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# o = autoencoder1(X)\n",
        "# X_embs = torch.squeeze(autoencoder1.y.detach())\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(12, 16),\n",
        "    nn.ReLU(),\n",
        "    # nn.Linear(48, 64),\n",
        "    nn.ReLU(),\n",
        "    # nn.Linear(64, 64),\n",
        "    # nn.ReLU(),\n",
        "    # nn.Linear(64, 32),\n",
        "    # nn.ReLU(),\n",
        "    nn.Linear(16, 1),\n",
        "    nn.Sigmoid(\n",
        "\n",
        "    )\n",
        ")\n",
        "model = model.cuda()\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "# import torch.optim as optim\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay= 1e-4)\n",
        "n_epochs = 1000\n",
        "batch_size = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X_emb_train), batch_size):\n",
        "        Xbatch = X_emb_train[i:i+batch_size]\n",
        "        y_pred = model(Xbatch)\n",
        "        ybatch = Y_train[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred.cuda(), ybatch.cuda())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #     print(loss)\n",
        "    # print(f'Finished epoch {epoch}, latest loss {loss}')\n",
        "# compute accuracy (no_grad is optional)\n",
        "    if(epoch%100==99):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(X_emb_train)\n",
        "            accuracy = (y_pred.round() == Y_train.cuda()).float().mean()\n",
        "            print(f\"Train Accuracy {accuracy}\",len(X_emb_train))\n",
        "            y_pred = model(X_emb_test)\n",
        "            accuracy = (y_pred.round() == Y_test.cuda()).float().mean()\n",
        "            print(f\"Test Accuracy {accuracy}\")\n",
        "            y_pred = model(X_emb_test2)\n",
        "            accuracy = (y_pred.round() == Y_test2.cuda()).float().mean()\n",
        "            # print(f\"Test Accuracy 2 {accuracy}\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     y_pred = model(X_emb_test)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    y_pred = model(X_emb_test2)\n",
        "    accuracy = (y_pred.round() == Y_ test2.cuda()).float().mean()\n",
        "    print(f\"Accuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# code to split test train \n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X[0:ind], Y[0:ind], test_size=0.2)\n",
        "X_test2 = X[ind:-1]\n",
        "Y_test2 = Y[ind:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.869694709777832\n",
            "Accuracy 0.7776087522506714\n"
          ]
        }
      ],
      "source": [
        "model1 = nn.Sequential(\n",
        "    nn.Linear(12, 48),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(48, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid(\n",
        "\n",
        "    )\n",
        ")\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
        "n_epochs = 100\n",
        "batch_size = 1000\n",
        " \n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        Xbatch = X_train[i:i+batch_size]\n",
        "        y_pred = model1(Xbatch)\n",
        "        ybatch = Y_train[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        optimizer1.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = model1(X_test)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")\n",
        "with torch.no_grad():\n",
        "    y_pred = model1(X_test2)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test2).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#jumble orders of X keeping corresponding Y same\n",
        "indices = torch.randperm(X.shape[0])\n",
        "X = X[indices]\n",
        "Y = Y[indices]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X[0:ind], Y[0:ind], test_size=0.2)\n",
        "X_test2 = X[ind:-1]\n",
        "Y_test2 = Y[ind:-1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.8577811121940613\n",
            "Accuracy 0.85123211145401\n"
          ]
        }
      ],
      "source": [
        "model1 = nn.Sequential(\n",
        "    nn.Linear(12, 48),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(48, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid(\n",
        "\n",
        "    )\n",
        ")\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
        "n_epochs = 100\n",
        "batch_size = 1000\n",
        " \n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        Xbatch = X_train[i:i+batch_size]\n",
        "        y_pred = model1(Xbatch)\n",
        "        ybatch = Y_train[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        optimizer1.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = model1(X_test)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")\n",
        "with torch.no_grad():\n",
        "    y_pred = model1(X_test2)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test2).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
